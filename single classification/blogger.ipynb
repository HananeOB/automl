{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataframe = pd.read_csv(\"blogger.csv\")\n",
    "dataframe.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   V1      100 non-null    object\n",
      " 1   V2      100 non-null    object\n",
      " 2   V3      100 non-null    object\n",
      " 3   V4      100 non-null    object\n",
      " 4   V5      100 non-null    object\n",
      " 5   Class   100 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dataframe[\"V5\"].unique()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['yes', 'no'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def function1(x) :\n",
    "    return {\n",
    "        'high'  : 1, \n",
    "        'medium': 2,\n",
    "        'low'   : 3\n",
    "    }[x]\n",
    "\n",
    "def function2(x) :\n",
    "    return {\n",
    "        'left'  : 1, \n",
    "        'middle': 2,\n",
    "        'right'   : 3\n",
    "    }[x]\n",
    "\n",
    "def function3(x) :\n",
    "    return {\n",
    "        'impression': 1, \n",
    "        'political' : 2,\n",
    "        'tourism'   : 3,\n",
    "        'news'      : 4,\n",
    "        'scientific': 5\n",
    "    }[x]\n",
    "\n",
    "def function4_5(x) :\n",
    "    return {\n",
    "        'yes'  : 1, \n",
    "        'no'   : 2,\n",
    "    }[x]\n",
    "\n",
    "dataframe[\"V1\"] = dataframe[\"V1\"].apply(function1)\n",
    "dataframe[\"V2\"] = dataframe[\"V2\"].apply(function2)\n",
    "dataframe[\"V3\"] = dataframe[\"V3\"].apply(function3)\n",
    "dataframe[\"V4\"] = dataframe[\"V4\"].apply(function4_5)\n",
    "dataframe[\"V5\"] = dataframe[\"V5\"].apply(function4_5)\n",
    "\n",
    "dataframe.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   V1  V2  V3  V4  V5  Class\n",
       "0   1   1   1   1   1      2\n",
       "1   1   1   2   1   1      2\n",
       "2   2   2   3   1   1      2\n",
       "3   1   1   2   1   1      2\n",
       "4   2   2   4   1   1      2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataframe.drop(columns=['Class'])\n",
    "y = dataframe['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Launch tpot training \n",
    "from tpot import TPOTClassifier\n",
    "clf_tpot = TPOTClassifier(verbosity=2, max_time_mins=30 )\n",
    "clf_tpot.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8625\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8625\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8625\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8625\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8625\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.875\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.875\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.875\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 26 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 27 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 28 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 29 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 30 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 31 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 32 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 33 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 34 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 35 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 36 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 37 - Current best internal CV score: 0.8875\n",
      "\n",
      "Generation 38 - Current best internal CV score: 0.8875\n",
      "\n",
      "30.00 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "                                                                                \n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=gini, max_features=0.2, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TPOTClassifier(max_time_mins=30, verbosity=2)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import autosklearn.classification\n",
    "clf_autosklearn = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=1800)\n",
    "clf_autosklearn.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AutoSklearnClassifier(per_run_time_limit=180, time_left_for_this_task=1800)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from oboe import AutoLearner, error\n",
    "import numpy as np \n",
    "xx_train = np.array(X_train)\n",
    "yy_train = np.array(y_train)\n",
    "method = 'Oboe' # 'Oboe' or 'TensorOboe'\n",
    "problem_type = 'classification'\n",
    "clf_oboe = AutoLearner(p_type=problem_type, runtime_limit=30, method=method, verbose=False)\n",
    "clf_oboe.fit(xx_train, yy_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ranks': [8, 9, 10, 11],\n",
       " 'runtime_limits': [1, 2, 4, 8],\n",
       " 'validation_loss': [0.5, 0.0, 0.0, 0.0, 0.0],\n",
       " 'filled_new_row': [array([[ 0.48908788,  0.46915072,  0.16906255, -0.01512139, -0.00564117,\n",
       "           0.48619201,  0.46293954,  0.17095659,  0.57888889, -0.00744805,\n",
       "           0.27144647,  0.28076001,  0.29801269,  0.31449283,  0.34537407,\n",
       "           0.39796174,  0.49599578,  0.58615521,  0.48494489,  0.43860807,\n",
       "           0.30067149,  0.27330889,  0.27144647,  0.27144647,  0.29190327,\n",
       "           0.29458868,  0.2993339 ,  0.30026215,  0.32830083,  0.32434139,\n",
       "           0.35924597,  0.35756692,  0.4119995 ,  0.41257931,  0.47000721,\n",
       "           0.47346459,  0.54859575,  0.55469293,  0.58576893,  0.58645213,\n",
       "           0.46968185,  0.47013852,  0.41300739,  0.41216192,  0.43125552,\n",
       "           0.43572012,  0.33680472,  0.33763966,  0.29471869,  0.2960302 ,\n",
       "           0.29190327,  0.29458868,  0.29190327,  0.29458868,  0.25214755,\n",
       "           0.23919332,  0.2555873 ,  0.22799412,  0.40788521,  0.42752039,\n",
       "           0.33000512,  0.37817503,  0.36833455,  0.39675396,  0.30401321,\n",
       "           0.3408708 ,  0.3408119 ,  0.36286689,  0.29083019,  0.31720823,\n",
       "           0.32469413,  0.33761918,  0.28682162,  0.30986865,  0.32163408,\n",
       "           0.33185651,  0.28970088,  0.31633919,  0.32836636,  0.34432893,\n",
       "           0.29144429,  0.32650606,  0.35702955,  0.24308583,  0.24257837,\n",
       "           0.32713265,  0.32399107,  0.35455107,  0.34774384,  0.36773978,\n",
       "           0.36233723,  0.37646969,  0.37399886,  0.38760583,  0.38194536,\n",
       "           0.39531972,  0.38969913,  0.40271282,  0.39977198,  0.44408987,\n",
       "           0.43440132,  0.4449928 ,  0.43429743,  0.41934047,  0.4264118 ,\n",
       "           0.42743609,  0.42638242,  0.41055621,  0.42007935,  0.42073894,\n",
       "           0.42227474,  0.40483491,  0.41685442,  0.41676244,  0.41958722,\n",
       "           0.39622297,  0.41014665,  0.41239124,  0.41721565,  0.39110313,\n",
       "           0.40735103,  0.41154389,  0.41635889,  0.38618701,  0.40323914,\n",
       "           0.40942518,  0.41461546,  0.38601221,  0.4007446 ,  0.40796034,\n",
       "           0.41333051,  0.48838019,  0.48834034,  0.43111582,  0.43093062,\n",
       "           0.44482596,  0.44520281,  0.27722278,  0.27761217,  0.28824746,\n",
       "           0.28849358,  0.26247613,  0.26315395,  0.39417974,  0.32176571,\n",
       "           0.31938133,  0.33175736,  0.32513716,  0.34767516,  0.34773424,\n",
       "           0.37438057,  0.37790209,  0.41679779,  0.42108917,  0.4830895 ,\n",
       "           0.49394185,  0.59624814,  0.60221533,  0.54981107,  0.55529617,\n",
       "           0.47158405,  0.46823403,  0.41250271,  0.41380113,  0.44314223,\n",
       "           0.44921131,  0.35186655,  0.34929844,  0.32417228,  0.32036643,\n",
       "           0.32176571,  0.31938133,  0.32176571,  0.31938133,  0.4812856 ,\n",
       "           0.4812856 ,  0.37790659,  0.28672799,  0.47398266,  0.47398266,\n",
       "           0.36679023,  0.28123963,  0.42827903,  0.42827903,  0.37424829,\n",
       "           0.2750805 ,  0.40315813,  0.40315813,  0.37942451,  0.27149558,\n",
       "           0.38707133,  0.38707133,  0.38233449,  0.27031068,  0.35309041,\n",
       "           0.35309041,  0.3752719 ,  0.26665601,  0.32447589,  0.32447589,\n",
       "           0.36334355,  0.29777778,  0.30527837,  0.30527837,  0.35225689,\n",
       "           0.30722222,  0.29104089,  0.29104089,  0.33670143,  0.24444444,\n",
       "           0.42641918,  0.41584929,  0.40943348,  0.40655819,  0.40573941,\n",
       "           0.40421692,  0.40182691,  0.39531906,  0.39097163]]),\n",
       "  array([[0.43529939, 0.41974032, 0.54592355, 0.56336058, 0.56848005,\n",
       "          0.42722845, 0.41144088, 0.54354526, 0.55484832, 0.55954942,\n",
       "          0.27686869, 0.28597084, 0.29890204, 0.32080251, 0.35670378,\n",
       "          0.40570032, 0.50200759, 0.60585576, 0.43833178, 0.44013906,\n",
       "          0.30362608, 0.27821562, 0.27686869, 0.27686869, 0.29017493,\n",
       "          0.2929035 , 0.29189527, 0.29446177, 0.32028062, 0.31516323,\n",
       "          0.35517572, 0.35162165, 0.41371561, 0.41213968, 0.48902383,\n",
       "          0.48711417, 0.57422669, 0.57604171, 0.6238542 , 0.62054289,\n",
       "          0.47282183, 0.47347096, 0.4633119 , 0.46359785, 0.46670638,\n",
       "          0.46888979, 0.33527843, 0.33454389, 0.28951158, 0.29179534,\n",
       "          0.29017493, 0.2929035 , 0.29017493, 0.2929035 , 0.41486063,\n",
       "          0.4195213 , 0.40515483, 0.39541956, 0.42454862, 0.47670181,\n",
       "          0.32201058, 0.3901213 , 0.35672109, 0.40960288, 0.28216759,\n",
       "          0.32560056, 0.31329715, 0.35456825, 0.26616393, 0.29531941,\n",
       "          0.29211731, 0.32016478, 0.26045795, 0.28346926, 0.28474187,\n",
       "          0.30398321, 0.26155923, 0.29048301, 0.29151457, 0.31395863,\n",
       "          0.26439142, 0.30106   , 0.42476092, 0.29856293, 0.29715023,\n",
       "          0.36357924, 0.3629914 , 0.38496297, 0.38113661, 0.39614372,\n",
       "          0.39016482, 0.40448051, 0.40193142, 0.41588269, 0.41071859,\n",
       "          0.4222593 , 0.41590896, 0.4271003 , 0.42478677, 0.42668535,\n",
       "          0.41852082, 0.43116772, 0.42207949, 0.40130813, 0.40935776,\n",
       "          0.41403329, 0.41264661, 0.39257879, 0.40289604, 0.40665854,\n",
       "          0.40803076, 0.38901741, 0.39968919, 0.40287904, 0.40538741,\n",
       "          0.38198204, 0.39423679, 0.399036  , 0.40345051, 0.37723048,\n",
       "          0.39142293, 0.39792586, 0.40252696, 0.37354269, 0.38834759,\n",
       "          0.39584718, 0.40085271, 0.37394686, 0.38646762, 0.39464711,\n",
       "          0.39966864, 0.55252772, 0.55249916, 0.43905601, 0.43893802,\n",
       "          0.45244526, 0.45291546, 0.26592907, 0.26641268, 0.2741179 ,\n",
       "          0.27444311, 0.25432539, 0.26444444, 0.41809432, 0.31294029,\n",
       "          0.3099075 , 0.31998723, 0.31423673, 0.33866965, 0.33579765,\n",
       "          0.36823586, 0.3689096 , 0.41521747, 0.41442793, 0.49484975,\n",
       "          0.50274929, 0.63689417, 0.64233082, 0.51439521, 0.52397072,\n",
       "          0.45657044, 0.45849062, 0.45297408, 0.45562041, 0.46691056,\n",
       "          0.47140749, 0.34375294, 0.34124257, 0.31362817, 0.31027463,\n",
       "          0.31294029, 0.3099075 , 0.31294029, 0.3099075 , 0.50095096,\n",
       "          0.50095096, 0.40771791, 0.29555556, 0.48614975, 0.48614975,\n",
       "          0.38849768, 0.30166667, 0.42117302, 0.42117302, 0.38317817,\n",
       "          0.30055556, 0.39167357, 0.39167357, 0.38078097, 0.39      ,\n",
       "          0.37360596, 0.37360596, 0.37923273, 0.30888889, 0.33606591,\n",
       "          0.33606591, 0.36911303, 0.26521725, 0.30789505, 0.30789505,\n",
       "          0.3557736 , 0.26258024, 0.28872435, 0.28872435, 0.34258375,\n",
       "          0.26296837, 0.29333333, 0.27166667, 0.32596401, 0.2615373 ,\n",
       "          0.40902538, 0.39976791, 0.39388061, 0.39112776, 0.39048025,\n",
       "          0.38893792, 0.38681881, 0.38263554, 0.38234945]]),\n",
       "  array([[0.37374209, 0.35731723, 0.50890953, 0.55696152, 0.57362133,\n",
       "          0.36525491, 0.35125898, 0.5135727 , 0.55274476, 0.56988512,\n",
       "          0.27946422, 0.28727085, 0.29810777, 0.3179458 , 0.3477269 ,\n",
       "          0.38652372, 0.4708901 , 0.55416576, 0.44185889, 0.46229083,\n",
       "          0.3038075 , 0.28073902, 0.27946422, 0.27946422, 0.30755609,\n",
       "          0.30818462, 0.30932006, 0.30993909, 0.33508991, 0.32857349,\n",
       "          0.3683639 , 0.36196557, 0.4214721 , 0.41665558, 0.49321345,\n",
       "          0.48716171, 0.56843858, 0.56595316, 0.60908241, 0.60248885,\n",
       "          0.51050638, 0.510396  , 0.51902862, 0.51995381, 0.48320527,\n",
       "          0.48110771, 0.35173973, 0.34890048, 0.30756162, 0.30715166,\n",
       "          0.30755609, 0.30818462, 0.30755609, 0.30818462, 0.55014785,\n",
       "          0.54267069, 0.54530845, 0.53221101, 0.43231121, 0.5017487 ,\n",
       "          0.32924846, 0.40643705, 0.36060613, 0.41448552, 0.28916873,\n",
       "          0.33460101, 0.31689238, 0.35604478, 0.27604389, 0.30416617,\n",
       "          0.29542374, 0.32048728, 0.26924705, 0.29171041, 0.28516759,\n",
       "          0.29894861, 0.27166667, 0.29249954, 0.28755515, 0.30168185,\n",
       "          0.29666667, 0.29546661, 0.41928739, 0.31133263, 0.31046405,\n",
       "          0.36767005, 0.36721159, 0.38755018, 0.38322557, 0.39657477,\n",
       "          0.39045533, 0.40362207, 0.40201092, 0.41478982, 0.40978868,\n",
       "          0.41994598, 0.41422208, 0.42771327, 0.4258377 , 0.42155937,\n",
       "          0.41370887, 0.42696195, 0.41815591, 0.39689122, 0.40475642,\n",
       "          0.40957049, 0.40913986, 0.38896335, 0.3984464 , 0.40185155,\n",
       "          0.40431335, 0.38548453, 0.39513729, 0.39790555, 0.40167314,\n",
       "          0.37911515, 0.3896164 , 0.39445984, 0.39939912, 0.3745216 ,\n",
       "          0.38655085, 0.39341127, 0.39836573, 0.37099053, 0.3827654 ,\n",
       "          0.39125435, 0.39687073, 0.37123833, 0.38117686, 0.39022251,\n",
       "          0.39550148, 0.57748043, 0.57746575, 0.44976677, 0.44968253,\n",
       "          0.45896366, 0.45944724, 0.28517739, 0.28569458, 0.2946109 ,\n",
       "          0.29486444, 0.27458996, 0.2745543 , 0.40318898, 0.32364592,\n",
       "          0.31852647, 0.32996783, 0.32213125, 0.34544345, 0.34226491,\n",
       "          0.37296262, 0.37082631, 0.41348668, 0.40925891, 0.48525231,\n",
       "          0.48797853, 0.59921238, 0.60186798, 0.5155907 , 0.52207108,\n",
       "          0.49084435, 0.49680002, 0.50945261, 0.51288684, 0.47300252,\n",
       "          0.4718819 , 0.35416372, 0.34949304, 0.32484721, 0.31891972,\n",
       "          0.32364592, 0.31852647, 0.32364592, 0.31852647, 0.50162563,\n",
       "          0.50162563, 0.41739321, 0.29890093, 0.47121914, 0.47121914,\n",
       "          0.37939106, 0.29396354, 0.41019069, 0.41019069, 0.35899441,\n",
       "          0.28955662, 0.38515759, 0.38515759, 0.35198311, 0.28709599,\n",
       "          0.37171069, 0.37171069, 0.34933266, 0.28659281, 0.34223256,\n",
       "          0.34223256, 0.34031212, 0.28315476, 0.31854795, 0.31854795,\n",
       "          0.33079694, 0.28038068, 0.30199954, 0.30199954, 0.32086874,\n",
       "          0.28113637, 0.29081079, 0.29081079, 0.30867654, 0.28012695,\n",
       "          0.40308017, 0.39376959, 0.38771389, 0.38493077, 0.38421136,\n",
       "          0.38239328, 0.37979626, 0.37584348, 0.37654681]]),\n",
       "  array([[0.34878483, 0.33433608, 0.48162622, 0.56214909, 0.57434908,\n",
       "          0.34125821, 0.32984626, 0.48583934, 0.55594967, 0.56870564,\n",
       "          0.27763888, 0.28283233, 0.28917737, 0.30478919, 0.33059318,\n",
       "          0.36754618, 0.44784358, 0.53745199, 0.44764315, 0.47232492,\n",
       "          0.29441556, 0.27861781, 0.27763888, 0.27763888, 0.30760008,\n",
       "          0.30844457, 0.30783669, 0.30816923, 0.3266835 , 0.32094254,\n",
       "          0.35433723, 0.34881787, 0.39963705, 0.39497141, 0.46714481,\n",
       "          0.46138332, 0.54059301, 0.53849556, 0.58276753, 0.57793793,\n",
       "          0.50280503, 0.5027188 , 0.51245353, 0.51267812, 0.44521354,\n",
       "          0.44405108, 0.34045849, 0.3380714 , 0.30944444, 0.30762325,\n",
       "          0.23555556, 0.30844457, 0.30760008, 0.30844457, 0.47078418,\n",
       "          0.479036  , 0.46000293, 0.46019517, 0.38935813, 0.45416966,\n",
       "          0.31273354, 0.37335211, 0.32847411, 0.38099723, 0.29666667,\n",
       "          0.31642578, 0.26055556, 0.33119406, 0.27333333, 0.27166667,\n",
       "          0.22166667, 0.26166667, 0.22833333, 0.26444444, 0.26444444,\n",
       "          0.25333333, 0.26590844, 0.25055556, 0.20444444, 0.25666667,\n",
       "          0.26737723, 0.23944444, 0.40528662, 0.32921911, 0.33172915,\n",
       "          0.36384185, 0.36826252, 0.37956751, 0.38116657, 0.38777065,\n",
       "          0.38798307, 0.39576667, 0.40022676, 0.40676266, 0.40822081,\n",
       "          0.41037531, 0.41202561, 0.41786857, 0.42301789, 0.41243096,\n",
       "          0.40488907, 0.41763601, 0.40900444, 0.38917753, 0.39677063,\n",
       "          0.40056268, 0.40035046, 0.38133999, 0.39067545, 0.39306265,\n",
       "          0.39570068, 0.37809505, 0.38759737, 0.38925245, 0.39314513,\n",
       "          0.37212192, 0.38250941, 0.3862316 , 0.39087676, 0.36790598,\n",
       "          0.37965976, 0.38530903, 0.39007805, 0.36492958, 0.37615592,\n",
       "          0.38343269, 0.38878897, 0.36528096, 0.37459926, 0.3824983 ,\n",
       "          0.38745243, 0.58534492, 0.58535095, 0.45858523, 0.45847537,\n",
       "          0.46209644, 0.46245504, 0.30656032, 0.30709789, 0.31479409,\n",
       "          0.31509484, 0.29430108, 0.29392026, 0.40752275, 0.31266497,\n",
       "          0.30819029, 0.31605739, 0.30992056, 0.32792698, 0.32503413,\n",
       "          0.35033992, 0.34848617, 0.38850181, 0.38430806, 0.45745188,\n",
       "          0.46072569, 0.56298717, 0.56615508, 0.49909985, 0.50624104,\n",
       "          0.48854825, 0.49230234, 0.50333672, 0.50586882, 0.43035175,\n",
       "          0.43046624, 0.33459623, 0.33044926, 0.31357127, 0.30840556,\n",
       "          0.31266497, 0.30819029, 0.31266497, 0.30819029, 0.51063069,\n",
       "          0.51063069, 0.41927487, 0.3062811 , 0.47641005, 0.47641005,\n",
       "          0.386144  , 0.30140472, 0.41120243, 0.41120243, 0.36594192,\n",
       "          0.29672845, 0.38381754, 0.38381754, 0.35684588, 0.29429865,\n",
       "          0.369417  , 0.369417  , 0.35240641, 0.29381875, 0.34031129,\n",
       "          0.34031129, 0.344101  , 0.29082344, 0.3192878 , 0.3192878 ,\n",
       "          0.33709485, 0.28802135, 0.33777778, 0.32444444, 0.32875139,\n",
       "          0.28867543, 0.29549081, 0.29549081, 0.31845533, 0.28786163,\n",
       "          0.39515773, 0.38597114, 0.3799942 , 0.37734383, 0.37670612,\n",
       "          0.37531437, 0.37303732, 0.37022152, 0.37330692]])],\n",
       " 'predicted_new_row': [array([[ 0.48908788,  0.46915072,  0.16906255, -0.01512139, -0.00564117,\n",
       "           0.48619201,  0.46293954,  0.17095659, -0.01879394, -0.00744805,\n",
       "           0.27144647,  0.28076001,  0.29801269,  0.31449283,  0.34537407,\n",
       "           0.39796174,  0.49599578,  0.58615521,  0.48494489,  0.43860807,\n",
       "           0.30067149,  0.27330889,  0.27144647,  0.27144647,  0.29190327,\n",
       "           0.29458868,  0.2993339 ,  0.30026215,  0.32830083,  0.32434139,\n",
       "           0.35924597,  0.35756692,  0.4119995 ,  0.41257931,  0.47000721,\n",
       "           0.47346459,  0.54859575,  0.55469293,  0.58576893,  0.58645213,\n",
       "           0.46968185,  0.47013852,  0.41300739,  0.41216192,  0.43125552,\n",
       "           0.43572012,  0.33680472,  0.33763966,  0.29471869,  0.2960302 ,\n",
       "           0.29190327,  0.29458868,  0.29190327,  0.29458868,  0.25214755,\n",
       "           0.23919332,  0.2555873 ,  0.22799412,  0.40788521,  0.42752039,\n",
       "           0.33000512,  0.37817503,  0.36833455,  0.39675396,  0.30401321,\n",
       "           0.3408708 ,  0.3408119 ,  0.36286689,  0.29083019,  0.31720823,\n",
       "           0.32469413,  0.33761918,  0.28682162,  0.30986865,  0.32163408,\n",
       "           0.33185651,  0.28970088,  0.31633919,  0.32836636,  0.34432893,\n",
       "           0.29144429,  0.32650606,  0.35702955,  0.24308583,  0.24257837,\n",
       "           0.32713265,  0.32399107,  0.35455107,  0.34774384,  0.36773978,\n",
       "           0.36233723,  0.37646969,  0.37399886,  0.38760583,  0.38194536,\n",
       "           0.39531972,  0.38969913,  0.40271282,  0.39977198,  0.44408987,\n",
       "           0.43440132,  0.4449928 ,  0.43429743,  0.41934047,  0.4264118 ,\n",
       "           0.42743609,  0.42638242,  0.41055621,  0.42007935,  0.42073894,\n",
       "           0.42227474,  0.40483491,  0.41685442,  0.41676244,  0.41958722,\n",
       "           0.39622297,  0.41014665,  0.41239124,  0.41721565,  0.39110313,\n",
       "           0.40735103,  0.41154389,  0.41635889,  0.38618701,  0.40323914,\n",
       "           0.40942518,  0.41461546,  0.38601221,  0.4007446 ,  0.40796034,\n",
       "           0.41333051,  0.48838019,  0.48834034,  0.43111582,  0.43093062,\n",
       "           0.44482596,  0.44520281,  0.27722278,  0.27761217,  0.28824746,\n",
       "           0.28849358,  0.26247613,  0.26315395,  0.39417974,  0.32176571,\n",
       "           0.31938133,  0.33175736,  0.32513716,  0.34767516,  0.34773424,\n",
       "           0.37438057,  0.37790209,  0.41679779,  0.42108917,  0.4830895 ,\n",
       "           0.49394185,  0.59624814,  0.60221533,  0.54981107,  0.55529617,\n",
       "           0.47158405,  0.46823403,  0.41250271,  0.41380113,  0.44314223,\n",
       "           0.44921131,  0.35186655,  0.34929844,  0.32417228,  0.32036643,\n",
       "           0.32176571,  0.31938133,  0.32176571,  0.31938133,  0.4812856 ,\n",
       "           0.4812856 ,  0.37790659,  0.28672799,  0.47398266,  0.47398266,\n",
       "           0.36679023,  0.28123963,  0.42827903,  0.42827903,  0.37424829,\n",
       "           0.2750805 ,  0.40315813,  0.40315813,  0.37942451,  0.27149558,\n",
       "           0.38707133,  0.38707133,  0.38233449,  0.27031068,  0.35309041,\n",
       "           0.35309041,  0.3752719 ,  0.26665601,  0.32447589,  0.32447589,\n",
       "           0.36334355,  0.26386962,  0.30527837,  0.30527837,  0.35225689,\n",
       "           0.26453455,  0.29104089,  0.29104089,  0.33670143,  0.26202549,\n",
       "           0.42641918,  0.41584929,  0.40943348,  0.40655819,  0.40573941,\n",
       "           0.40421692,  0.40182691,  0.39531906,  0.39097163]]),\n",
       "  array([[0.43529939, 0.41974032, 0.54592355, 0.56336058, 0.56848005,\n",
       "          0.42722845, 0.41144088, 0.54354526, 0.55484832, 0.55954942,\n",
       "          0.27686869, 0.28597084, 0.29890204, 0.32080251, 0.35670378,\n",
       "          0.40570032, 0.50200759, 0.60585576, 0.43833178, 0.44013906,\n",
       "          0.30362608, 0.27821562, 0.27686869, 0.27686869, 0.29017493,\n",
       "          0.2929035 , 0.29189527, 0.29446177, 0.32028062, 0.31516323,\n",
       "          0.35517572, 0.35162165, 0.41371561, 0.41213968, 0.48902383,\n",
       "          0.48711417, 0.57422669, 0.57604171, 0.6238542 , 0.62054289,\n",
       "          0.47282183, 0.47347096, 0.4633119 , 0.46359785, 0.46670638,\n",
       "          0.46888979, 0.33527843, 0.33454389, 0.28951158, 0.29179534,\n",
       "          0.29017493, 0.2929035 , 0.29017493, 0.2929035 , 0.41486063,\n",
       "          0.4195213 , 0.40515483, 0.39541956, 0.42454862, 0.47670181,\n",
       "          0.32201058, 0.3901213 , 0.35672109, 0.40960288, 0.28216759,\n",
       "          0.32560056, 0.31329715, 0.35456825, 0.26616393, 0.29531941,\n",
       "          0.29211731, 0.32016478, 0.26045795, 0.28346926, 0.28474187,\n",
       "          0.30398321, 0.26155923, 0.29048301, 0.29151457, 0.31395863,\n",
       "          0.26439142, 0.30106   , 0.42476092, 0.29856293, 0.29715023,\n",
       "          0.36357924, 0.3629914 , 0.38496297, 0.38113661, 0.39614372,\n",
       "          0.39016482, 0.40448051, 0.40193142, 0.41588269, 0.41071859,\n",
       "          0.4222593 , 0.41590896, 0.4271003 , 0.42478677, 0.42668535,\n",
       "          0.41852082, 0.43116772, 0.42207949, 0.40130813, 0.40935776,\n",
       "          0.41403329, 0.41264661, 0.39257879, 0.40289604, 0.40665854,\n",
       "          0.40803076, 0.38901741, 0.39968919, 0.40287904, 0.40538741,\n",
       "          0.38198204, 0.39423679, 0.399036  , 0.40345051, 0.37723048,\n",
       "          0.39142293, 0.39792586, 0.40252696, 0.37354269, 0.38834759,\n",
       "          0.39584718, 0.40085271, 0.37394686, 0.38646762, 0.39464711,\n",
       "          0.39966864, 0.55252772, 0.55249916, 0.43905601, 0.43893802,\n",
       "          0.45244526, 0.45291546, 0.26592907, 0.26641268, 0.2741179 ,\n",
       "          0.27444311, 0.25432539, 0.25464167, 0.41809432, 0.31294029,\n",
       "          0.3099075 , 0.31998723, 0.31423673, 0.33866965, 0.33579765,\n",
       "          0.36823586, 0.3689096 , 0.41521747, 0.41442793, 0.49484975,\n",
       "          0.50274929, 0.63689417, 0.64233082, 0.51439521, 0.52397072,\n",
       "          0.45657044, 0.45849062, 0.45297408, 0.45562041, 0.46691056,\n",
       "          0.47140749, 0.34375294, 0.34124257, 0.31362817, 0.31027463,\n",
       "          0.31294029, 0.3099075 , 0.31294029, 0.3099075 , 0.50095096,\n",
       "          0.50095096, 0.40771791, 0.28016684, 0.48614975, 0.48614975,\n",
       "          0.38849768, 0.27512065, 0.42117302, 0.42117302, 0.38317817,\n",
       "          0.27049985, 0.39167357, 0.39167357, 0.38078097, 0.26807505,\n",
       "          0.37360596, 0.37360596, 0.37923273, 0.26782101, 0.33606591,\n",
       "          0.33606591, 0.36911303, 0.26521725, 0.30789505, 0.30789505,\n",
       "          0.3557736 , 0.26258024, 0.28872435, 0.28872435, 0.34258375,\n",
       "          0.26296837, 0.27665013, 0.27665013, 0.32596401, 0.2615373 ,\n",
       "          0.40902538, 0.39976791, 0.39388061, 0.39112776, 0.39048025,\n",
       "          0.38893792, 0.38681881, 0.38263554, 0.38234945]]),\n",
       "  array([[0.37374209, 0.35731723, 0.50890953, 0.55696152, 0.57362133,\n",
       "          0.36525491, 0.35125898, 0.5135727 , 0.55274476, 0.56988512,\n",
       "          0.27946422, 0.28727085, 0.29810777, 0.3179458 , 0.3477269 ,\n",
       "          0.38652372, 0.4708901 , 0.55416576, 0.44185889, 0.46229083,\n",
       "          0.3038075 , 0.28073902, 0.27946422, 0.27946422, 0.30755609,\n",
       "          0.30818462, 0.30932006, 0.30993909, 0.33508991, 0.32857349,\n",
       "          0.3683639 , 0.36196557, 0.4214721 , 0.41665558, 0.49321345,\n",
       "          0.48716171, 0.56843858, 0.56595316, 0.60908241, 0.60248885,\n",
       "          0.51050638, 0.510396  , 0.51902862, 0.51995381, 0.48320527,\n",
       "          0.48110771, 0.35173973, 0.34890048, 0.30756162, 0.30715166,\n",
       "          0.30755609, 0.30818462, 0.30755609, 0.30818462, 0.55014785,\n",
       "          0.54267069, 0.54530845, 0.53221101, 0.43231121, 0.5017487 ,\n",
       "          0.32924846, 0.40643705, 0.36060613, 0.41448552, 0.28916873,\n",
       "          0.33460101, 0.31689238, 0.35604478, 0.27604389, 0.30416617,\n",
       "          0.29542374, 0.32048728, 0.26924705, 0.29171041, 0.28516759,\n",
       "          0.29894861, 0.2681433 , 0.29249954, 0.28755515, 0.30168185,\n",
       "          0.26972039, 0.29546661, 0.41928739, 0.31133263, 0.31046405,\n",
       "          0.36767005, 0.36721159, 0.38755018, 0.38322557, 0.39657477,\n",
       "          0.39045533, 0.40362207, 0.40201092, 0.41478982, 0.40978868,\n",
       "          0.41994598, 0.41422208, 0.42771327, 0.4258377 , 0.42155937,\n",
       "          0.41370887, 0.42696195, 0.41815591, 0.39689122, 0.40475642,\n",
       "          0.40957049, 0.40913986, 0.38896335, 0.3984464 , 0.40185155,\n",
       "          0.40431335, 0.38548453, 0.39513729, 0.39790555, 0.40167314,\n",
       "          0.37911515, 0.3896164 , 0.39445984, 0.39939912, 0.3745216 ,\n",
       "          0.38655085, 0.39341127, 0.39836573, 0.37099053, 0.3827654 ,\n",
       "          0.39125435, 0.39687073, 0.37123833, 0.38117686, 0.39022251,\n",
       "          0.39550148, 0.57748043, 0.57746575, 0.44976677, 0.44968253,\n",
       "          0.45896366, 0.45944724, 0.28517739, 0.28569458, 0.2946109 ,\n",
       "          0.29486444, 0.27458996, 0.2745543 , 0.40318898, 0.32364592,\n",
       "          0.31852647, 0.32996783, 0.32213125, 0.34544345, 0.34226491,\n",
       "          0.37296262, 0.37082631, 0.41348668, 0.40925891, 0.48525231,\n",
       "          0.48797853, 0.59921238, 0.60186798, 0.5155907 , 0.52207108,\n",
       "          0.49084435, 0.49680002, 0.50945261, 0.51288684, 0.47300252,\n",
       "          0.4718819 , 0.35416372, 0.34949304, 0.32484721, 0.31891972,\n",
       "          0.32364592, 0.31852647, 0.32364592, 0.31852647, 0.50162563,\n",
       "          0.50162563, 0.41739321, 0.29890093, 0.47121914, 0.47121914,\n",
       "          0.37939106, 0.29396354, 0.41019069, 0.41019069, 0.35899441,\n",
       "          0.28955662, 0.38515759, 0.38515759, 0.35198311, 0.28709599,\n",
       "          0.37171069, 0.37171069, 0.34933266, 0.28659281, 0.34223256,\n",
       "          0.34223256, 0.34031212, 0.28315476, 0.31854795, 0.31854795,\n",
       "          0.33079694, 0.28038068, 0.30199954, 0.30199954, 0.32086874,\n",
       "          0.28113637, 0.29081079, 0.29081079, 0.30867654, 0.28012695,\n",
       "          0.40308017, 0.39376959, 0.38771389, 0.38493077, 0.38421136,\n",
       "          0.38239328, 0.37979626, 0.37584348, 0.37654681]]),\n",
       "  array([[0.34878483, 0.33433608, 0.48162622, 0.56214909, 0.57434908,\n",
       "          0.34125821, 0.32984626, 0.48583934, 0.55594967, 0.56870564,\n",
       "          0.27763888, 0.28283233, 0.28917737, 0.30478919, 0.33059318,\n",
       "          0.36754618, 0.44784358, 0.53745199, 0.44764315, 0.47232492,\n",
       "          0.29441556, 0.27861781, 0.27763888, 0.27763888, 0.30760008,\n",
       "          0.30844457, 0.30783669, 0.30816923, 0.3266835 , 0.32094254,\n",
       "          0.35433723, 0.34881787, 0.39963705, 0.39497141, 0.46714481,\n",
       "          0.46138332, 0.54059301, 0.53849556, 0.58276753, 0.57793793,\n",
       "          0.50280503, 0.5027188 , 0.51245353, 0.51267812, 0.44521354,\n",
       "          0.44405108, 0.34045849, 0.3380714 , 0.30741337, 0.30762325,\n",
       "          0.30760008, 0.30844457, 0.30760008, 0.30844457, 0.47078418,\n",
       "          0.479036  , 0.46000293, 0.46019517, 0.38935813, 0.45416966,\n",
       "          0.31273354, 0.37335211, 0.32847411, 0.38099723, 0.28198642,\n",
       "          0.31642578, 0.29219217, 0.33119406, 0.271391  , 0.29355629,\n",
       "          0.27492029, 0.30187729, 0.26646275, 0.28567662, 0.26874343,\n",
       "          0.2870566 , 0.26590844, 0.28903024, 0.27284949, 0.29245421,\n",
       "          0.26737723, 0.29532146, 0.40528662, 0.32921911, 0.33172915,\n",
       "          0.36384185, 0.36826252, 0.37956751, 0.38116657, 0.38777065,\n",
       "          0.38798307, 0.39576667, 0.40022676, 0.40676266, 0.40822081,\n",
       "          0.41037531, 0.41202561, 0.41786857, 0.42301789, 0.41243096,\n",
       "          0.40488907, 0.41763601, 0.40900444, 0.38917753, 0.39677063,\n",
       "          0.40056268, 0.40035046, 0.38133999, 0.39067545, 0.39306265,\n",
       "          0.39570068, 0.37809505, 0.38759737, 0.38925245, 0.39314513,\n",
       "          0.37212192, 0.38250941, 0.3862316 , 0.39087676, 0.36790598,\n",
       "          0.37965976, 0.38530903, 0.39007805, 0.36492958, 0.37615592,\n",
       "          0.38343269, 0.38878897, 0.36528096, 0.37459926, 0.3824983 ,\n",
       "          0.38745243, 0.58534492, 0.58535095, 0.45858523, 0.45847537,\n",
       "          0.46209644, 0.46245504, 0.30656032, 0.30709789, 0.31479409,\n",
       "          0.31509484, 0.29430108, 0.29392026, 0.40752275, 0.31266497,\n",
       "          0.30819029, 0.31605739, 0.30992056, 0.32792698, 0.32503413,\n",
       "          0.35033992, 0.34848617, 0.38850181, 0.38430806, 0.45745188,\n",
       "          0.46072569, 0.56298717, 0.56615508, 0.49909985, 0.50624104,\n",
       "          0.48854825, 0.49230234, 0.50333672, 0.50586882, 0.43035175,\n",
       "          0.43046624, 0.33459623, 0.33044926, 0.31357127, 0.30840556,\n",
       "          0.31266497, 0.30819029, 0.31266497, 0.30819029, 0.51063069,\n",
       "          0.51063069, 0.41927487, 0.3062811 , 0.47641005, 0.47641005,\n",
       "          0.386144  , 0.30140472, 0.41120243, 0.41120243, 0.36594192,\n",
       "          0.29672845, 0.38381754, 0.38381754, 0.35684588, 0.29429865,\n",
       "          0.369417  , 0.369417  , 0.35240641, 0.29381875, 0.34031129,\n",
       "          0.34031129, 0.344101  , 0.29082344, 0.3192878 , 0.3192878 ,\n",
       "          0.33709485, 0.28802135, 0.30460031, 0.30460031, 0.32875139,\n",
       "          0.28867543, 0.29549081, 0.29549081, 0.31845533, 0.28786163,\n",
       "          0.39515773, 0.38597114, 0.3799942 , 0.37734383, 0.37670612,\n",
       "          0.37531437, 0.37303732, 0.37022152, 0.37330692]])],\n",
       " 'actual_runtimes': [2.3358185291290283,\n",
       "  4.448511123657227,\n",
       "  8.15960955619812,\n",
       "  17.21853804588318],\n",
       " 'sampled_indices': [{8,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   95,\n",
       "   100,\n",
       "   104,\n",
       "   120,\n",
       "   128,\n",
       "   143,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   180,\n",
       "   184,\n",
       "   188,\n",
       "   197,\n",
       "   201,\n",
       "   205,\n",
       "   209,\n",
       "   210},\n",
       "  {8,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   87,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   100,\n",
       "   104,\n",
       "   108,\n",
       "   110,\n",
       "   112,\n",
       "   116,\n",
       "   120,\n",
       "   128,\n",
       "   142,\n",
       "   143,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   180,\n",
       "   181,\n",
       "   184,\n",
       "   185,\n",
       "   188,\n",
       "   189,\n",
       "   192,\n",
       "   193,\n",
       "   196,\n",
       "   197,\n",
       "   201,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211},\n",
       "  {8,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   76,\n",
       "   80,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   87,\n",
       "   89,\n",
       "   91,\n",
       "   93,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   99,\n",
       "   100,\n",
       "   104,\n",
       "   108,\n",
       "   110,\n",
       "   112,\n",
       "   116,\n",
       "   118,\n",
       "   120,\n",
       "   122,\n",
       "   126,\n",
       "   128,\n",
       "   142,\n",
       "   143,\n",
       "   157,\n",
       "   160,\n",
       "   161,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   184,\n",
       "   185,\n",
       "   188,\n",
       "   189,\n",
       "   192,\n",
       "   193,\n",
       "   196,\n",
       "   197,\n",
       "   200,\n",
       "   201,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211},\n",
       "  {3,\n",
       "   8,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   48,\n",
       "   50,\n",
       "   54,\n",
       "   64,\n",
       "   66,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   99,\n",
       "   100,\n",
       "   104,\n",
       "   108,\n",
       "   110,\n",
       "   112,\n",
       "   114,\n",
       "   116,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   124,\n",
       "   126,\n",
       "   128,\n",
       "   130,\n",
       "   142,\n",
       "   143,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   165,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   188,\n",
       "   189,\n",
       "   192,\n",
       "   193,\n",
       "   196,\n",
       "   197,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211}],\n",
       " 'models': [<oboe.ensemble.Ensemble at 0x7fb1069ef8e0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7fb1069ef8e0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7fb1069ef8e0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7fb1069ef8e0>]}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"tpot score:\\t\\t \", clf_tpot.score(X_test, y_test))\n",
    "print(\"autosklern score:\\t\\t \", clf_autosklearn.score(X_test, y_test))\n",
    "\n",
    "xx_test = np.array(X_test)\n",
    "yy_test = np.array(y_test)\n",
    "y_predicted = clf_oboe.predict(xx_test)[0]\n",
    "print(\"oboe score:\\t\\t \", accuracy_score(y_test, y_predicted) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tpot score:\t\t  0.8\n",
      "autosklern score:\t\t  0.85\n",
      "oboe score:\t\t  0.85\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "tpot_pred    = clf_tpot.predict(X_test)\n",
    "sklearn_pred = clf_autosklearn.predict(X_test)\n",
    "oboe_pred    = clf_oboe.predict(X_test)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "prob = pd.DataFrame(list(zip(sklearn_pred, oboe_pred, tpot_pred)), columns=[ \"AutoSklearn_Class\", \"Oboe_Class\", \"TPOT_Class\"])\n",
    "def function(x) :\n",
    "    if [x['AutoSklearn_Class'],x['Oboe_Class'], x['TPOT_Class']].count(1)>=2 :\n",
    "        return 1\n",
    "    else   :\n",
    "        return 2\n",
    "    # else :\n",
    "    #     int(x['AutoSklearn_Class'])\n",
    "prob['ensemble'] = prob.apply(function, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(accuracy_score(y_test.astype(int), prob['ensemble'] ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f0305a4bd922d78a1a9933c10651bf50f37f6183f11a48783a3fae819d3386ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}