{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataframe = pd.read_csv(\"fertility.csv\")\n",
    "dataframe.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      100 non-null    float64\n",
      " 1   V2      100 non-null    float64\n",
      " 2   V3      100 non-null    int64  \n",
      " 3   V4      100 non-null    int64  \n",
      " 4   V5      100 non-null    int64  \n",
      " 5   V6      100 non-null    int64  \n",
      " 6   V7      100 non-null    float64\n",
      " 7   V8      100 non-null    int64  \n",
      " 8   V9      100 non-null    float64\n",
      " 9   Class   100 non-null    int64  \n",
      "dtypes: float64(4), int64(6)\n",
      "memory usage: 7.9 KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataframe.drop(columns=['Class'])\n",
    "y = dataframe['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Launch tpot training \n",
    "from tpot import TPOTClassifier\n",
    "clf_tpot = TPOTClassifier(verbosity=2, max_time_mins=30 )\n",
    "clf_tpot.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 26 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 27 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 28 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 29 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 30 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 31 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 32 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 33 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 34 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 35 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 36 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 37 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 38 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 39 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 40 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 41 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 42 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 43 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 44 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 45 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 46 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 47 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 48 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 49 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 50 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 51 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 52 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 53 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 54 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 55 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 56 - Current best internal CV score: 0.9125\n",
      "\n",
      "Generation 57 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 58 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 59 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 60 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 61 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 62 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 63 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 64 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 65 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 66 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 67 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 68 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 69 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 70 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 71 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 72 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 73 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 74 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 75 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 76 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 77 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 78 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 79 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 80 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 81 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 82 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 83 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 84 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 85 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 86 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 87 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 88 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 89 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 90 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 91 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 92 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 93 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 94 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 95 - Current best internal CV score: 0.925\n",
      "\n",
      "Generation 96 - Current best internal CV score: 0.925\n",
      "\n",
      "30.02 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "                                                                                \n",
      "Best pipeline: DecisionTreeClassifier(SGDClassifier(input_matrix, alpha=0.001, eta0=0.1, fit_intercept=False, l1_ratio=0.5, learning_rate=constant, loss=modified_huber, penalty=elasticnet, power_t=0.0), criterion=entropy, max_depth=9, min_samples_leaf=7, min_samples_split=4)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TPOTClassifier(max_time_mins=30, verbosity=2)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import autosklearn.classification\n",
    "clf_autosklearn = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=1800)\n",
    "clf_autosklearn.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WARNING] [2021-08-26 20:07:38,017:Client-EnsembleBuilder] No models better than random - using Dummy loss!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
      "[WARNING] [2021-08-26 20:07:39,661:Client-EnsembleBuilder] No models better than random - using Dummy loss!Number of models besides current dummy model: 2. Number of dummy models: 1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AutoSklearnClassifier(per_run_time_limit=180, time_left_for_this_task=1800)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from oboe import AutoLearner, error\n",
    "import numpy as np \n",
    "xx_train = np.array(X_train)\n",
    "yy_train = np.array(y_train)\n",
    "method = 'Oboe' # 'Oboe' or 'TensorOboe'\n",
    "problem_type = 'classification'\n",
    "clf_oboe = AutoLearner(p_type=problem_type, runtime_limit=30, method=method, verbose=False)\n",
    "clf_oboe.fit(xx_train, yy_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ranks': [8, 9, 10, 11],\n",
       " 'runtime_limits': [1, 2, 4, 8],\n",
       " 'validation_loss': [0.5,\n",
       "  0.30000000000000004,\n",
       "  0.30000000000000004,\n",
       "  0.30000000000000004,\n",
       "  0.30000000000000004],\n",
       " 'filled_new_row': [array([[0.5207338 , 0.51922718, 0.70788483, 0.77059112, 0.76614192,\n",
       "          0.51935237, 0.51521816, 0.70124169, 0.75369216, 0.74791374,\n",
       "          0.41813511, 0.42331033, 0.42960151, 0.4389839 , 0.45353953,\n",
       "          0.47750798, 0.5031187 , 0.50848337, 0.472599  , 0.51574954,\n",
       "          0.43258998, 0.41886664, 0.41813511, 0.41813511, 0.42238371,\n",
       "          0.42388334, 0.4185672 , 0.42115624, 0.43073769, 0.43002987,\n",
       "          0.44442034, 0.44490785, 0.47007196, 0.47023565, 0.49755372,\n",
       "          0.49884815, 0.50902824, 0.51037554, 0.50231706, 0.50206429,\n",
       "          0.4782555 , 0.4799244 , 0.51939115, 0.51993922, 0.50879123,\n",
       "          0.51096561, 0.44327682, 0.44462433, 0.42089828, 0.42257949,\n",
       "          0.42238371, 0.42388334, 0.42238371, 0.42388334, 0.46220616,\n",
       "          0.48422106, 0.43856865, 0.44991283, 0.48038062, 0.50490642,\n",
       "          0.42781312, 0.45258573, 0.44016968, 0.47719092, 0.40155495,\n",
       "          0.4236243 , 0.41489221, 0.44564517, 0.3889203 , 0.40486079,\n",
       "          0.40313459, 0.42736262, 0.35984848, 0.41742424, 0.40292912,\n",
       "          0.42286019, 0.38563971, 0.40713909, 0.41084685, 0.43312215,\n",
       "          0.39000092, 0.42293222, 0.50712356, 0.45733964, 0.4598906 ,\n",
       "          0.4847419 , 0.48920801, 0.4879561 , 0.49049967, 0.4918157 ,\n",
       "          0.49326411, 0.49225832, 0.49583531, 0.49575039, 0.49898735,\n",
       "          0.49835983, 0.50100205, 0.49959716, 0.50370023, 0.50973615,\n",
       "          0.50377007, 0.51291323, 0.50684676, 0.49970172, 0.49971452,\n",
       "          0.50587485, 0.50298691, 0.4965028 , 0.49713347, 0.5042806 ,\n",
       "          0.5019552 , 0.49543743, 0.49603899, 0.50324014, 0.50106846,\n",
       "          0.49278738, 0.49380395, 0.50154509, 0.50121238, 0.49095807,\n",
       "          0.49300178, 0.50122323, 0.50107145, 0.48886171, 0.49212742,\n",
       "          0.50050598, 0.50085808, 0.48904846, 0.49154929, 0.49963673,\n",
       "          0.50071433, 0.5855797 , 0.58555021, 0.49499558, 0.4949245 ,\n",
       "          0.50448394, 0.50477685, 0.41134463, 0.41184057, 0.41802025,\n",
       "          0.41816302, 0.40163208, 0.40208503, 0.53298547, 0.42733331,\n",
       "          0.42570019, 0.42830624, 0.42615816, 0.43581609, 0.43672794,\n",
       "          0.4483441 , 0.45079002, 0.47082446, 0.47193099, 0.50064947,\n",
       "          0.50392418, 0.51204243, 0.51622797, 0.4584044 , 0.46467075,\n",
       "          0.48674749, 0.48987599, 0.52645585, 0.52810487, 0.50745725,\n",
       "          0.51422206, 0.43902204, 0.43952403, 0.4267063 , 0.42568491,\n",
       "          0.42733331, 0.42570019, 0.42733331, 0.42570019, 0.5141332 ,\n",
       "          0.5141332 , 0.50245255, 0.42403814, 0.51793545, 0.51793545,\n",
       "          0.49724621, 0.42102946, 0.49842077, 0.49842077, 0.49372839,\n",
       "          0.41661627, 0.48306364, 0.48306364, 0.4940438 , 0.41526642,\n",
       "          0.47250427, 0.47250427, 0.49186137, 0.41505272, 0.44682986,\n",
       "          0.44682986, 0.48565605, 0.41246588, 0.42968351, 0.42968351,\n",
       "          0.48095715, 0.41666667, 0.4197573 , 0.4197573 , 0.4757217 ,\n",
       "          0.4114155 , 0.41280483, 0.41280483, 0.46752811, 0.41187351,\n",
       "          0.50306098, 0.49996833, 0.49778681, 0.49725614, 0.49704922,\n",
       "          0.49677226, 0.49656809, 0.49531315, 0.49863162]]),\n",
       "  array([[0.54081136, 0.53985996, 0.67486628, 0.71227413, 0.70553944,\n",
       "          0.54043988, 0.53590496, 0.66655718, 0.6937733 , 0.68563548,\n",
       "          0.41692112, 0.42216682, 0.42901606, 0.43767967, 0.45247419,\n",
       "          0.47862425, 0.50457373, 0.5067793 , 0.47420703, 0.51302421,\n",
       "          0.43137171, 0.41766318, 0.41692112, 0.41692112, 0.42192743,\n",
       "          0.42413482, 0.41837899, 0.42158653, 0.43014288, 0.43039491,\n",
       "          0.44270525, 0.44460614, 0.4672165 , 0.46883556, 0.49140253,\n",
       "          0.49467799, 0.49978838, 0.50277933, 0.48897658, 0.49031271,\n",
       "          0.46527202, 0.46732856, 0.50198701, 0.50226823, 0.49975434,\n",
       "          0.50361133, 0.44154041, 0.44386417, 0.42056876, 0.42300793,\n",
       "          0.42192743, 0.42413482, 0.42192743, 0.42413482, 0.39915896,\n",
       "          0.42899162, 0.5       , 0.5       , 0.4715417 , 0.4916671 ,\n",
       "          0.42467374, 0.44581169, 0.43703781, 0.47476089, 0.40071862,\n",
       "          0.42325114, 0.41423248, 0.44665561, 0.38783039, 0.40544346,\n",
       "          0.40367551, 0.42969032, 0.38368148, 0.40211838, 0.40472463,\n",
       "          0.42771755, 0.45075758, 0.40944025, 0.41376205, 0.44030963,\n",
       "          0.39035135, 0.4280322 , 0.5035886 , 0.46024247, 0.46313677,\n",
       "          0.49025464, 0.49487642, 0.49344472, 0.49655489, 0.49759342,\n",
       "          0.50001838, 0.49799298, 0.50228388, 0.5013183 , 0.50521363,\n",
       "          0.5038017 , 0.50732796, 0.50442227, 0.50917928, 0.51178996,\n",
       "          0.50555823, 0.51390699, 0.50795444, 0.50236057, 0.50171624,\n",
       "          0.50727271, 0.50438772, 0.49913604, 0.49922949, 0.50605345,\n",
       "          0.50359821, 0.49790913, 0.49820204, 0.50508303, 0.50278229,\n",
       "          0.49514338, 0.49592405, 0.50335652, 0.50300035, 0.49337027,\n",
       "          0.49523524, 0.50308463, 0.50294413, 0.49117417, 0.49446369,\n",
       "          0.50245197, 0.50276035, 0.491372  , 0.49378114, 0.50153304,\n",
       "          0.50267873, 0.5775183 , 0.57749169, 0.49413884, 0.49403801,\n",
       "          0.50377226, 0.50403797, 0.4124278 , 0.41290753, 0.41928886,\n",
       "          0.41943965, 0.40206825, 0.40258233, 0.53543227, 0.4268245 ,\n",
       "          0.42624434, 0.42783771, 0.42667739, 0.43519089, 0.43700635,\n",
       "          0.44718011, 0.45071541, 0.46943127, 0.47215348, 0.49729362,\n",
       "          0.50222312, 0.50279074, 0.5079236 , 0.45142324, 0.45827124,\n",
       "          0.48019991, 0.48168272, 0.51105789, 0.51228548, 0.50049205,\n",
       "          0.50935827, 0.43744888, 0.43880696, 0.42619421, 0.4262325 ,\n",
       "          0.4268245 , 0.42624434, 0.4268245 , 0.42624434, 0.51364307,\n",
       "          0.51364307, 0.49944546, 0.42465704, 0.52362549, 0.52362549,\n",
       "          0.49970545, 0.42163217, 0.50729395, 0.50729395, 0.5006951 ,\n",
       "          0.41701698, 0.49148315, 0.49148315, 0.50307845, 0.41561278,\n",
       "          0.4799369 , 0.4799369 , 0.50166447, 0.41533196, 0.45200424,\n",
       "          0.45200424, 0.49645371, 0.4128734 , 0.4334942 , 0.4334942 ,\n",
       "          0.49247571, 0.41107955, 0.42293116, 0.42293116, 0.48752353,\n",
       "          0.50909091, 0.41556342, 0.41556342, 0.47922673, 0.41207413,\n",
       "          0.50544988, 0.50234237, 0.50029904, 0.49985222, 0.49966415,\n",
       "          0.49952789, 0.49946185, 0.49812926, 0.50125844]]),\n",
       "  array([[0.59373626, 0.59454858, 0.64181221, 0.55624084, 0.55970508,\n",
       "          0.59544816, 0.59082047, 0.63002837, 0.53504841, 0.53846222,\n",
       "          0.42063251, 0.4272733 , 0.43786828, 0.44522676, 0.45598319,\n",
       "          0.48076041, 0.50551934, 0.49364032, 0.48082808, 0.51852254,\n",
       "          0.44207601, 0.42185641, 0.42063251, 0.42063251, 0.42870665,\n",
       "          0.43113858, 0.45      , 0.43168674, 0.44077801, 0.44313079,\n",
       "          0.45261742, 0.45629418, 0.47462797, 0.47821121, 0.48892096,\n",
       "          0.49469057, 0.49394412, 0.49848138, 0.48416292, 0.48541344,\n",
       "          0.47198122, 0.47454244, 0.51395121, 0.51537824, 0.51402821,\n",
       "          0.51884228, 0.45469636, 0.45902195, 0.50833333, 0.43060595,\n",
       "          0.42870665, 0.43113858, 0.42870665, 0.43113858, 0.50191903,\n",
       "          0.52092795, 0.48637055, 0.50372656, 0.4865443 , 0.51711997,\n",
       "          0.43997583, 0.46302086, 0.45699116, 0.49299231, 0.41464003,\n",
       "          0.44001276, 0.4386575 , 0.4673641 , 0.44393939, 0.41976119,\n",
       "          0.42907099, 0.45094335, 0.39602256, 0.41435774, 0.42982992,\n",
       "          0.44886669, 0.39810785, 0.41899776, 0.43852235, 0.46334144,\n",
       "          0.45151515, 0.4373284 , 0.50837623, 0.44862189, 0.45198917,\n",
       "          0.4929257 , 0.49653846, 0.49630097, 0.49862763, 0.49917407,\n",
       "          0.50159383, 0.49703141, 0.50173224, 0.49936064, 0.50371988,\n",
       "          0.50423844, 0.5074237 , 0.50821932, 0.51285532, 0.51247992,\n",
       "          0.50293492, 0.51280061, 0.50529574, 0.50370704, 0.50006047,\n",
       "          0.50625564, 0.50292303, 0.50196767, 0.49853203, 0.5054228 ,\n",
       "          0.5025104 , 0.50061347, 0.49757846, 0.50476535, 0.50194027,\n",
       "          0.49823907, 0.49573364, 0.50272168, 0.50236067, 0.49706242,\n",
       "          0.4953491 , 0.50272065, 0.50198931, 0.4945289 , 0.49466818,\n",
       "          0.50193078, 0.50192728, 0.49443236, 0.49406035, 0.50102736,\n",
       "          0.50193926, 0.55905671, 0.55903778, 0.47776362, 0.47773074,\n",
       "          0.49053226, 0.49080747, 0.42176216, 0.42231801, 0.42815582,\n",
       "          0.42814226, 0.41667178, 0.41783213, 0.51683998, 0.43574051,\n",
       "          0.43572949, 0.43835221, 0.43759567, 0.44451598, 0.44821938,\n",
       "          0.45565377, 0.46154426, 0.47315834, 0.47982191, 0.49517061,\n",
       "          0.50004934, 0.49881887, 0.50391127, 0.46340909, 0.46776275,\n",
       "          0.49288219, 0.49846039, 0.53944977, 0.54224319, 0.51550057,\n",
       "          0.52583081, 0.45095933, 0.45390124, 0.43570107, 0.43635578,\n",
       "          0.43574051, 0.43572949, 0.43574051, 0.43572949, 0.5064184 ,\n",
       "          0.5064184 , 0.51608683, 0.45114676, 0.51412646, 0.51412646,\n",
       "          0.50853882, 0.45037635, 0.51652402, 0.51652402, 0.50699786,\n",
       "          0.44719217, 0.50750466, 0.50750466, 0.5088345 , 0.44599386,\n",
       "          0.49994878, 0.49994878, 0.50786389, 0.44530004, 0.47925146,\n",
       "          0.47925146, 0.50153762, 0.4423047 , 0.46352753, 0.46352753,\n",
       "          0.49823801, 0.44081069, 0.45406687, 0.45406687, 0.49453822,\n",
       "          0.44198614, 0.44557415, 0.44557415, 0.48770187, 0.44226822,\n",
       "          0.50505608, 0.50310558, 0.50195082, 0.50177207, 0.50166021,\n",
       "          0.50208308, 0.50224554, 0.49995629, 0.50157355]]),\n",
       "  array([[0.53341083, 0.53015709, 0.36060606, 0.27938041, 0.24318182,\n",
       "          0.53697917, 0.53016192, 0.41666667, 0.37878788, 0.20151515,\n",
       "          0.4188668 , 0.42490488, 0.43611081, 0.44132547, 0.44677137,\n",
       "          0.46268607, 0.48598522, 0.4852734 , 0.50050601, 0.50233424,\n",
       "          0.43884604, 0.41997384, 0.4188668 , 0.4188668 , 0.4490763 ,\n",
       "          0.45123692, 0.45255485, 0.45373054, 0.4659242 , 0.46704195,\n",
       "          0.4761913 , 0.47753279, 0.49178592, 0.4943806 , 0.49552995,\n",
       "          0.50078005, 0.50014826, 0.50354762, 0.49511871, 0.49596943,\n",
       "          0.4908355 , 0.49246254, 0.49476902, 0.49533787, 0.50718676,\n",
       "          0.5101805 , 0.47622428, 0.47967244, 0.45088684, 0.4517625 ,\n",
       "          0.5       , 0.45123692, 0.4490763 , 0.45123692, 0.48543812,\n",
       "          0.49264814, 0.48267956, 0.49605548, 0.48549951, 0.51396103,\n",
       "          0.44899084, 0.4849845 , 0.46683487, 0.49908694, 0.42753853,\n",
       "          0.46579395, 0.45476773, 0.48118179, 0.41694304, 0.44429684,\n",
       "          0.44766153, 0.46621288, 0.41211348, 0.43599495, 0.44372946,\n",
       "          0.45739851, 0.41133856, 0.43221784, 0.446437  , 0.46414951,\n",
       "          0.41402214, 0.44017672, 0.46204848, 0.43847653, 0.44072694,\n",
       "          0.49178459, 0.49315253, 0.50111862, 0.5002037 , 0.5040416 ,\n",
       "          0.50470328, 0.50016068, 0.50345135, 0.50065973, 0.50358482,\n",
       "          0.50489612, 0.50691399, 0.50766267, 0.51108706, 0.50929696,\n",
       "          0.50138269, 0.5069121 , 0.50309614, 0.50307685, 0.50006363,\n",
       "          0.50286564, 0.5018659 , 0.50225477, 0.4991222 , 0.50270861,\n",
       "          0.50183139, 0.50068483, 0.49866603, 0.5025056 , 0.50145404,\n",
       "          0.49854685, 0.49682608, 0.50122626, 0.50161587, 0.49744158,\n",
       "          0.49651485, 0.5016605 , 0.50136559, 0.49464685, 0.49528371,\n",
       "          0.50116743, 0.50145545, 0.49435255, 0.49493385, 0.50040168,\n",
       "          0.50141753, 0.49899985, 0.49899616, 0.47520289, 0.47510267,\n",
       "          0.48497987, 0.48519868, 0.43908446, 0.43962114, 0.44625484,\n",
       "          0.44610869, 0.43570561, 0.43682013, 0.48417001, 0.45497777,\n",
       "          0.45458457, 0.46032111, 0.45721731, 0.46450518, 0.46920161,\n",
       "          0.47363292, 0.47913528, 0.48350461, 0.4893224 , 0.49325844,\n",
       "          0.49587013, 0.49280536, 0.4946819 , 0.50628805, 0.50569257,\n",
       "          0.50673445, 0.50729336, 0.50647858, 0.50801609, 0.50549637,\n",
       "          0.5117752 , 0.47034042, 0.4711073 , 0.45616217, 0.45549468,\n",
       "          0.45497777, 0.45458457, 0.45497777, 0.45458457, 0.5053778 ,\n",
       "          0.5053778 , 0.50735824, 0.4693274 , 0.51464041, 0.51464041,\n",
       "          0.50518993, 0.46858356, 0.52497166, 0.52497166, 0.50775516,\n",
       "          0.46529121, 0.52054524, 0.52054524, 0.51064015, 0.46377295,\n",
       "          0.51580638, 0.51580638, 0.51107021, 0.46290666, 0.50054246,\n",
       "          0.50054246, 0.50604099, 0.4591718 , 0.48641669, 0.48641669,\n",
       "          0.50299886, 0.45754582, 0.4782618 , 0.4782618 , 0.5007411 ,\n",
       "          0.4592086 , 0.4688135 , 0.4688135 , 0.49609076, 0.45878337,\n",
       "          0.50470973, 0.50296874, 0.50187006, 0.50167694, 0.50137791,\n",
       "          0.50139581, 0.50081244, 0.49753193, 0.49701725]])],\n",
       " 'predicted_new_row': [array([[0.5207338 , 0.51922718, 0.70788483, 0.77059112, 0.76614192,\n",
       "          0.51935237, 0.51521816, 0.70124169, 0.75369216, 0.74791374,\n",
       "          0.41813511, 0.42331033, 0.42960151, 0.4389839 , 0.45353953,\n",
       "          0.47750798, 0.5031187 , 0.50848337, 0.472599  , 0.51574954,\n",
       "          0.43258998, 0.41886664, 0.41813511, 0.41813511, 0.42238371,\n",
       "          0.42388334, 0.4185672 , 0.42115624, 0.43073769, 0.43002987,\n",
       "          0.44442034, 0.44490785, 0.47007196, 0.47023565, 0.49755372,\n",
       "          0.49884815, 0.50902824, 0.51037554, 0.50231706, 0.50206429,\n",
       "          0.4782555 , 0.4799244 , 0.51939115, 0.51993922, 0.50879123,\n",
       "          0.51096561, 0.44327682, 0.44462433, 0.42089828, 0.42257949,\n",
       "          0.42238371, 0.42388334, 0.42238371, 0.42388334, 0.46220616,\n",
       "          0.48422106, 0.43856865, 0.44991283, 0.48038062, 0.50490642,\n",
       "          0.42781312, 0.45258573, 0.44016968, 0.47719092, 0.40155495,\n",
       "          0.4236243 , 0.41489221, 0.44564517, 0.3889203 , 0.40486079,\n",
       "          0.40313459, 0.42736262, 0.38430436, 0.40100712, 0.40292912,\n",
       "          0.42286019, 0.38563971, 0.40713909, 0.41084685, 0.43312215,\n",
       "          0.39000092, 0.42293222, 0.50712356, 0.45733964, 0.4598906 ,\n",
       "          0.4847419 , 0.48920801, 0.4879561 , 0.49049967, 0.4918157 ,\n",
       "          0.49326411, 0.49225832, 0.49583531, 0.49575039, 0.49898735,\n",
       "          0.49835983, 0.50100205, 0.49959716, 0.50370023, 0.50973615,\n",
       "          0.50377007, 0.51291323, 0.50684676, 0.49970172, 0.49971452,\n",
       "          0.50587485, 0.50298691, 0.4965028 , 0.49713347, 0.5042806 ,\n",
       "          0.5019552 , 0.49543743, 0.49603899, 0.50324014, 0.50106846,\n",
       "          0.49278738, 0.49380395, 0.50154509, 0.50121238, 0.49095807,\n",
       "          0.49300178, 0.50122323, 0.50107145, 0.48886171, 0.49212742,\n",
       "          0.50050598, 0.50085808, 0.48904846, 0.49154929, 0.49963673,\n",
       "          0.50071433, 0.5855797 , 0.58555021, 0.49499558, 0.4949245 ,\n",
       "          0.50448394, 0.50477685, 0.41134463, 0.41184057, 0.41802025,\n",
       "          0.41816302, 0.40163208, 0.40208503, 0.53298547, 0.42733331,\n",
       "          0.42570019, 0.42830624, 0.42615816, 0.43581609, 0.43672794,\n",
       "          0.4483441 , 0.45079002, 0.47082446, 0.47193099, 0.50064947,\n",
       "          0.50392418, 0.51204243, 0.51622797, 0.4584044 , 0.46467075,\n",
       "          0.48674749, 0.48987599, 0.52645585, 0.52810487, 0.50745725,\n",
       "          0.51422206, 0.43902204, 0.43952403, 0.4267063 , 0.42568491,\n",
       "          0.42733331, 0.42570019, 0.42733331, 0.42570019, 0.5141332 ,\n",
       "          0.5141332 , 0.50245255, 0.42403814, 0.51793545, 0.51793545,\n",
       "          0.49724621, 0.42102946, 0.49842077, 0.49842077, 0.49372839,\n",
       "          0.41661627, 0.48306364, 0.48306364, 0.4940438 , 0.41526642,\n",
       "          0.47250427, 0.47250427, 0.49186137, 0.41505272, 0.44682986,\n",
       "          0.44682986, 0.48565605, 0.41246588, 0.42968351, 0.42968351,\n",
       "          0.48095715, 0.41062711, 0.4197573 , 0.4197573 , 0.4757217 ,\n",
       "          0.4114155 , 0.41280483, 0.41280483, 0.46752811, 0.41187351,\n",
       "          0.50306098, 0.49996833, 0.49778681, 0.49725614, 0.49704922,\n",
       "          0.49677226, 0.49656809, 0.49531315, 0.49863162]]),\n",
       "  array([[0.54081136, 0.53985996, 0.67486628, 0.71227413, 0.70553944,\n",
       "          0.54043988, 0.53590496, 0.66655718, 0.6937733 , 0.68563548,\n",
       "          0.41692112, 0.42216682, 0.42901606, 0.43767967, 0.45247419,\n",
       "          0.47862425, 0.50457373, 0.5067793 , 0.47420703, 0.51302421,\n",
       "          0.43137171, 0.41766318, 0.41692112, 0.41692112, 0.42192743,\n",
       "          0.42413482, 0.41837899, 0.42158653, 0.43014288, 0.43039491,\n",
       "          0.44270525, 0.44460614, 0.4672165 , 0.46883556, 0.49140253,\n",
       "          0.49467799, 0.49978838, 0.50277933, 0.48897658, 0.49031271,\n",
       "          0.46527202, 0.46732856, 0.50198701, 0.50226823, 0.49975434,\n",
       "          0.50361133, 0.44154041, 0.44386417, 0.42056876, 0.42300793,\n",
       "          0.42192743, 0.42413482, 0.42192743, 0.42413482, 0.39915896,\n",
       "          0.42899162, 0.37456667, 0.39104576, 0.4715417 , 0.4916671 ,\n",
       "          0.42467374, 0.44581169, 0.43703781, 0.47476089, 0.40071862,\n",
       "          0.42325114, 0.41423248, 0.44665561, 0.38783039, 0.40544346,\n",
       "          0.40367551, 0.42969032, 0.38368148, 0.40211838, 0.40472463,\n",
       "          0.42771755, 0.3858287 , 0.40944025, 0.41376205, 0.44030963,\n",
       "          0.39035135, 0.4280322 , 0.5035886 , 0.46024247, 0.46313677,\n",
       "          0.49025464, 0.49487642, 0.49344472, 0.49655489, 0.49759342,\n",
       "          0.50001838, 0.49799298, 0.50228388, 0.5013183 , 0.50521363,\n",
       "          0.5038017 , 0.50732796, 0.50442227, 0.50917928, 0.51178996,\n",
       "          0.50555823, 0.51390699, 0.50795444, 0.50236057, 0.50171624,\n",
       "          0.50727271, 0.50438772, 0.49913604, 0.49922949, 0.50605345,\n",
       "          0.50359821, 0.49790913, 0.49820204, 0.50508303, 0.50278229,\n",
       "          0.49514338, 0.49592405, 0.50335652, 0.50300035, 0.49337027,\n",
       "          0.49523524, 0.50308463, 0.50294413, 0.49117417, 0.49446369,\n",
       "          0.50245197, 0.50276035, 0.491372  , 0.49378114, 0.50153304,\n",
       "          0.50267873, 0.5775183 , 0.57749169, 0.49413884, 0.49403801,\n",
       "          0.50377226, 0.50403797, 0.4124278 , 0.41290753, 0.41928886,\n",
       "          0.41943965, 0.40206825, 0.40258233, 0.53543227, 0.4268245 ,\n",
       "          0.42624434, 0.42783771, 0.42667739, 0.43519089, 0.43700635,\n",
       "          0.44718011, 0.45071541, 0.46943127, 0.47215348, 0.49729362,\n",
       "          0.50222312, 0.50279074, 0.5079236 , 0.45142324, 0.45827124,\n",
       "          0.48019991, 0.48168272, 0.51105789, 0.51228548, 0.50049205,\n",
       "          0.50935827, 0.43744888, 0.43880696, 0.42619421, 0.4262325 ,\n",
       "          0.4268245 , 0.42624434, 0.4268245 , 0.42624434, 0.51364307,\n",
       "          0.51364307, 0.49944546, 0.42465704, 0.52362549, 0.52362549,\n",
       "          0.49970545, 0.42163217, 0.50729395, 0.50729395, 0.5006951 ,\n",
       "          0.41701698, 0.49148315, 0.49148315, 0.50307845, 0.41561278,\n",
       "          0.4799369 , 0.4799369 , 0.50166447, 0.41533196, 0.45200424,\n",
       "          0.45200424, 0.49645371, 0.4128734 , 0.4334942 , 0.4334942 ,\n",
       "          0.49247571, 0.41107955, 0.42293116, 0.42293116, 0.48752353,\n",
       "          0.41176541, 0.41556342, 0.41556342, 0.47922673, 0.41207413,\n",
       "          0.50544988, 0.50234237, 0.50029904, 0.49985222, 0.49966415,\n",
       "          0.49952789, 0.49946185, 0.49812926, 0.50125844]]),\n",
       "  array([[0.59373626, 0.59454858, 0.64181221, 0.55624084, 0.55970508,\n",
       "          0.59544816, 0.59082047, 0.63002837, 0.53504841, 0.53846222,\n",
       "          0.42063251, 0.4272733 , 0.43786828, 0.44522676, 0.45598319,\n",
       "          0.48076041, 0.50551934, 0.49364032, 0.48082808, 0.51852254,\n",
       "          0.44207601, 0.42185641, 0.42063251, 0.42063251, 0.42870665,\n",
       "          0.43113858, 0.42780485, 0.43168674, 0.44077801, 0.44313079,\n",
       "          0.45261742, 0.45629418, 0.47462797, 0.47821121, 0.48892096,\n",
       "          0.49469057, 0.49394412, 0.49848138, 0.48416292, 0.48541344,\n",
       "          0.47198122, 0.47454244, 0.51395121, 0.51537824, 0.51402821,\n",
       "          0.51884228, 0.45469636, 0.45902195, 0.42842499, 0.43060595,\n",
       "          0.42870665, 0.43113858, 0.42870665, 0.43113858, 0.50191903,\n",
       "          0.52092795, 0.48637055, 0.50372656, 0.4865443 , 0.51711997,\n",
       "          0.43997583, 0.46302086, 0.45699116, 0.49299231, 0.41464003,\n",
       "          0.44001276, 0.4386575 , 0.4673641 , 0.4005927 , 0.41976119,\n",
       "          0.42907099, 0.45094335, 0.39602256, 0.41435774, 0.42982992,\n",
       "          0.44886669, 0.39810785, 0.41899776, 0.43852235, 0.46334144,\n",
       "          0.40226864, 0.4373284 , 0.50837623, 0.44862189, 0.45198917,\n",
       "          0.4929257 , 0.49653846, 0.49630097, 0.49862763, 0.49917407,\n",
       "          0.50159383, 0.49703141, 0.50173224, 0.49936064, 0.50371988,\n",
       "          0.50423844, 0.5074237 , 0.50821932, 0.51285532, 0.51247992,\n",
       "          0.50293492, 0.51280061, 0.50529574, 0.50370704, 0.50006047,\n",
       "          0.50625564, 0.50292303, 0.50196767, 0.49853203, 0.5054228 ,\n",
       "          0.5025104 , 0.50061347, 0.49757846, 0.50476535, 0.50194027,\n",
       "          0.49823907, 0.49573364, 0.50272168, 0.50236067, 0.49706242,\n",
       "          0.4953491 , 0.50272065, 0.50198931, 0.4945289 , 0.49466818,\n",
       "          0.50193078, 0.50192728, 0.49443236, 0.49406035, 0.50102736,\n",
       "          0.50193926, 0.55905671, 0.55903778, 0.47776362, 0.47773074,\n",
       "          0.49053226, 0.49080747, 0.42176216, 0.42231801, 0.42815582,\n",
       "          0.42814226, 0.41667178, 0.41783213, 0.51683998, 0.43574051,\n",
       "          0.43572949, 0.43835221, 0.43759567, 0.44451598, 0.44821938,\n",
       "          0.45565377, 0.46154426, 0.47315834, 0.47982191, 0.49517061,\n",
       "          0.50004934, 0.49881887, 0.50391127, 0.46340909, 0.46776275,\n",
       "          0.49288219, 0.49846039, 0.53944977, 0.54224319, 0.51550057,\n",
       "          0.52583081, 0.45095933, 0.45390124, 0.43570107, 0.43635578,\n",
       "          0.43574051, 0.43572949, 0.43574051, 0.43572949, 0.5064184 ,\n",
       "          0.5064184 , 0.51608683, 0.45114676, 0.51412646, 0.51412646,\n",
       "          0.50853882, 0.45037635, 0.51652402, 0.51652402, 0.50699786,\n",
       "          0.44719217, 0.50750466, 0.50750466, 0.5088345 , 0.44599386,\n",
       "          0.49994878, 0.49994878, 0.50786389, 0.44530004, 0.47925146,\n",
       "          0.47925146, 0.50153762, 0.4423047 , 0.46352753, 0.46352753,\n",
       "          0.49823801, 0.44081069, 0.45406687, 0.45406687, 0.49453822,\n",
       "          0.44198614, 0.44557415, 0.44557415, 0.48770187, 0.44226822,\n",
       "          0.50505608, 0.50310558, 0.50195082, 0.50177207, 0.50166021,\n",
       "          0.50208308, 0.50224554, 0.49995629, 0.50157355]]),\n",
       "  array([[0.53341083, 0.53015709, 0.40961917, 0.27938041, 0.28781764,\n",
       "          0.53697917, 0.53016192, 0.41052807, 0.27089693, 0.2797154 ,\n",
       "          0.4188668 , 0.42490488, 0.43611081, 0.44132547, 0.44677137,\n",
       "          0.46268607, 0.48598522, 0.4852734 , 0.50050601, 0.50233424,\n",
       "          0.43884604, 0.41997384, 0.4188668 , 0.4188668 , 0.4490763 ,\n",
       "          0.45123692, 0.45255485, 0.45373054, 0.4659242 , 0.46704195,\n",
       "          0.4761913 , 0.47753279, 0.49178592, 0.4943806 , 0.49552995,\n",
       "          0.50078005, 0.50014826, 0.50354762, 0.49511871, 0.49596943,\n",
       "          0.4908355 , 0.49246254, 0.49476902, 0.49533787, 0.50718676,\n",
       "          0.5101805 , 0.47622428, 0.47967244, 0.45088684, 0.4517625 ,\n",
       "          0.4490763 , 0.45123692, 0.4490763 , 0.45123692, 0.48543812,\n",
       "          0.49264814, 0.48267956, 0.49605548, 0.48549951, 0.51396103,\n",
       "          0.44899084, 0.4849845 , 0.46683487, 0.49908694, 0.42753853,\n",
       "          0.46579395, 0.45476773, 0.48118179, 0.41694304, 0.44429684,\n",
       "          0.44766153, 0.46621288, 0.41211348, 0.43599495, 0.44372946,\n",
       "          0.45739851, 0.41133856, 0.43221784, 0.446437  , 0.46414951,\n",
       "          0.41402214, 0.44017672, 0.46204848, 0.43847653, 0.44072694,\n",
       "          0.49178459, 0.49315253, 0.50111862, 0.5002037 , 0.5040416 ,\n",
       "          0.50470328, 0.50016068, 0.50345135, 0.50065973, 0.50358482,\n",
       "          0.50489612, 0.50691399, 0.50766267, 0.51108706, 0.50929696,\n",
       "          0.50138269, 0.5069121 , 0.50309614, 0.50307685, 0.50006363,\n",
       "          0.50286564, 0.5018659 , 0.50225477, 0.4991222 , 0.50270861,\n",
       "          0.50183139, 0.50068483, 0.49866603, 0.5025056 , 0.50145404,\n",
       "          0.49854685, 0.49682608, 0.50122626, 0.50161587, 0.49744158,\n",
       "          0.49651485, 0.5016605 , 0.50136559, 0.49464685, 0.49528371,\n",
       "          0.50116743, 0.50145545, 0.49435255, 0.49493385, 0.50040168,\n",
       "          0.50141753, 0.49899985, 0.49899616, 0.47520289, 0.47510267,\n",
       "          0.48497987, 0.48519868, 0.43908446, 0.43962114, 0.44625484,\n",
       "          0.44610869, 0.43570561, 0.43682013, 0.48417001, 0.45497777,\n",
       "          0.45458457, 0.46032111, 0.45721731, 0.46450518, 0.46920161,\n",
       "          0.47363292, 0.47913528, 0.48350461, 0.4893224 , 0.49325844,\n",
       "          0.49587013, 0.49280536, 0.4946819 , 0.50628805, 0.50569257,\n",
       "          0.50673445, 0.50729336, 0.50647858, 0.50801609, 0.50549637,\n",
       "          0.5117752 , 0.47034042, 0.4711073 , 0.45616217, 0.45549468,\n",
       "          0.45497777, 0.45458457, 0.45497777, 0.45458457, 0.5053778 ,\n",
       "          0.5053778 , 0.50735824, 0.4693274 , 0.51464041, 0.51464041,\n",
       "          0.50518993, 0.46858356, 0.52497166, 0.52497166, 0.50775516,\n",
       "          0.46529121, 0.52054524, 0.52054524, 0.51064015, 0.46377295,\n",
       "          0.51580638, 0.51580638, 0.51107021, 0.46290666, 0.50054246,\n",
       "          0.50054246, 0.50604099, 0.4591718 , 0.48641669, 0.48641669,\n",
       "          0.50299886, 0.45754582, 0.4782618 , 0.4782618 , 0.5007411 ,\n",
       "          0.4592086 , 0.4688135 , 0.4688135 , 0.49609076, 0.45878337,\n",
       "          0.50470973, 0.50296874, 0.50187006, 0.50167694, 0.50137791,\n",
       "          0.50139581, 0.50081244, 0.49753193, 0.49701725]])],\n",
       " 'actual_runtimes': [2.5764822959899902,\n",
       "  5.353369235992432,\n",
       "  10.33529257774353,\n",
       "  18.140019416809082],\n",
       " 'sampled_indices': [{10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   72,\n",
       "   73,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   95,\n",
       "   100,\n",
       "   104,\n",
       "   120,\n",
       "   128,\n",
       "   143,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   180,\n",
       "   184,\n",
       "   188,\n",
       "   197,\n",
       "   201,\n",
       "   210},\n",
       "  {10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   56,\n",
       "   57,\n",
       "   72,\n",
       "   73,\n",
       "   76,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   87,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   100,\n",
       "   104,\n",
       "   108,\n",
       "   110,\n",
       "   112,\n",
       "   116,\n",
       "   120,\n",
       "   128,\n",
       "   143,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   180,\n",
       "   184,\n",
       "   188,\n",
       "   192,\n",
       "   196,\n",
       "   197,\n",
       "   201,\n",
       "   205,\n",
       "   210,\n",
       "   211},\n",
       "  {10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   26,\n",
       "   48,\n",
       "   56,\n",
       "   57,\n",
       "   68,\n",
       "   72,\n",
       "   73,\n",
       "   76,\n",
       "   80,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   87,\n",
       "   89,\n",
       "   91,\n",
       "   93,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   99,\n",
       "   100,\n",
       "   104,\n",
       "   108,\n",
       "   110,\n",
       "   112,\n",
       "   116,\n",
       "   118,\n",
       "   120,\n",
       "   122,\n",
       "   126,\n",
       "   128,\n",
       "   143,\n",
       "   157,\n",
       "   160,\n",
       "   161,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   184,\n",
       "   188,\n",
       "   192,\n",
       "   193,\n",
       "   196,\n",
       "   197,\n",
       "   200,\n",
       "   201,\n",
       "   204,\n",
       "   205,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211},\n",
       "  {2,\n",
       "   3,\n",
       "   4,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   26,\n",
       "   48,\n",
       "   50,\n",
       "   54,\n",
       "   56,\n",
       "   57,\n",
       "   68,\n",
       "   72,\n",
       "   73,\n",
       "   76,\n",
       "   80,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   99,\n",
       "   100,\n",
       "   104,\n",
       "   108,\n",
       "   110,\n",
       "   112,\n",
       "   114,\n",
       "   116,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   124,\n",
       "   126,\n",
       "   128,\n",
       "   130,\n",
       "   143,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   165,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   188,\n",
       "   192,\n",
       "   193,\n",
       "   196,\n",
       "   197,\n",
       "   200,\n",
       "   201,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211}],\n",
       " 'models': [<oboe.ensemble.Ensemble at 0x7fda4d42a8e0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7fda4d42a8e0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7fda4d42a8e0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7fda4d42a8e0>]}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"tpot score:\\t\\t \", clf_tpot.score(X_test, y_test))\n",
    "print(\"autosklern score:\\t\\t \", clf_autosklearn.score(X_test, y_test))\n",
    "\n",
    "xx_test = np.array(X_test)\n",
    "yy_test = np.array(y_test)\n",
    "y_predicted = clf_oboe.predict(xx_test)[0]\n",
    "print(\"oboe score:\\t\\t \", accuracy_score(y_test, y_predicted) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tpot score:\t\t  0.95\n",
      "autosklern score:\t\t  0.85\n",
      "oboe score:\t\t  0.6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tpot_pred    = clf_tpot.predict(X_test)\n",
    "sklearn_pred = clf_autosklearn.predict(X_test)\n",
    "oboe_pred    = clf_oboe.predict(X_test)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "prob = pd.DataFrame(list(zip(sklearn_pred, oboe_pred, tpot_pred)), columns=[ \"AutoSklearn_Class\", \"Oboe_Class\", \"TPOT_Class\"])\n",
    "def function(x) :\n",
    "    if [x['AutoSklearn_Class'],x['Oboe_Class'], x['TPOT_Class']].count(1)>=2 :\n",
    "        return 1\n",
    "    else   :\n",
    "        return 2\n",
    "    # else :\n",
    "    #     int(x['AutoSklearn_Class'])\n",
    "prob['ensemble'] = prob.apply(function, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(accuracy_score(y_test.astype(int), prob['ensemble'] ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f0305a4bd922d78a1a9933c10651bf50f37f6183f11a48783a3fae819d3386ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}