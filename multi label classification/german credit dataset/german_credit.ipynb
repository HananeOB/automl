{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import data to pandas dataframe \n",
    "import pandas as pd \n",
    "german_dataset = pd.read_csv(\"german.data-numeric\", header=None, sep=\"  \")\n",
    "german_dataset.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-b77f8c0dfe5e>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  german_dataset = pd.read_csv(\"german.data-numeric\", header=None, sep=\"  \")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  15  16  17  18  19  20  21  \\\n",
       "0   1   6   4  12   5   5   3   4   1  67  ...   0   0   1   0   0   1   0   \n",
       "1   2  48   2  60   1   3   2   2   1  22  ...   0   0   1   0   0   1   0   \n",
       "2   4  12   4  21   1   4   3   3   1  49  ...   0   0   1   0   0   1   0   \n",
       "3   1  42   2  79   1   4   3   4   2  45  ...   0   0   0   0   0   0   0   \n",
       "4   1  24   3  49   1   3   3   4   4  53  ...   1   0   1   0   0   0   0   \n",
       "\n",
       "   22  23   24  \n",
       "0   0   1  1.0  \n",
       "1   0   1  2.0  \n",
       "2   1   0  1.0  \n",
       "3   0   1  1.0  \n",
       "4   0   1  2.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Remove missing data \n",
    "german_dataset = german_dataset.dropna()\n",
    "german_dataset.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 959 entries, 0 to 999\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       959 non-null    int64  \n",
      " 1   1       959 non-null    int64  \n",
      " 2   2       959 non-null    object \n",
      " 3   3       959 non-null    int64  \n",
      " 4   4       959 non-null    int64  \n",
      " 5   5       959 non-null    int64  \n",
      " 6   6       959 non-null    int64  \n",
      " 7   7       959 non-null    int64  \n",
      " 8   8       959 non-null    int64  \n",
      " 9   9       959 non-null    int64  \n",
      " 10  10      959 non-null    int64  \n",
      " 11  11      959 non-null    int64  \n",
      " 12  12      959 non-null    int64  \n",
      " 13  13      959 non-null    int64  \n",
      " 14  14      959 non-null    int64  \n",
      " 15  15      959 non-null    int64  \n",
      " 16  16      959 non-null    int64  \n",
      " 17  17      959 non-null    int64  \n",
      " 18  18      959 non-null    int64  \n",
      " 19  19      959 non-null    int64  \n",
      " 20  20      959 non-null    int64  \n",
      " 21  21      959 non-null    int64  \n",
      " 22  22      959 non-null    int64  \n",
      " 23  23      959 non-null    int64  \n",
      " 24  24      959 non-null    float64\n",
      "dtypes: float64(1), int64(23), object(1)\n",
      "memory usage: 194.8+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Split data to train (80%) and test (20%) data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X = german_dataset.iloc[:,:-1]\n",
    "y = german_dataset.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Launch tpot \n",
    "from tpot import TPOTClassifier\n",
    "clf_german = TPOTClassifier(verbosity=2, max_time_mins=30)\n",
    "clf_german.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7836007130124777\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7836007130124777\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7836007130124777\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.7913844325609031\n",
      "\n",
      "30.19 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "                                                                                \n",
      "Best pipeline: LogisticRegression(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False), C=10.0, dual=False, penalty=l2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TPOTClassifier(max_time_mins=30, verbosity=2)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Auto sklearn does not accept object data \n",
    "X_train.iloc[:,2] = X_train.iloc[:,2].astype(int)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Launch autosklearn \n",
    "import autosklearn.classification\n",
    "clf_autosklearn = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=1800)\n",
    "clf_autosklearn.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AutoSklearnClassifier(per_run_time_limit=180, time_left_for_this_task=1800)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Launch OBOE \n",
    "from oboe import AutoLearner, error\n",
    "import numpy as np \n",
    "x_train = np.array(X_train)\n",
    "yy_train = np.array(y_train)\n",
    "method = 'Oboe' # 'Oboe' or 'TensorOboe'\n",
    "problem_type = 'classification'\n",
    "clf_oboe = AutoLearner(p_type=problem_type, runtime_limit=30, method=method, verbose=False)\n",
    "clf_oboe.fit(x_train, yy_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/datagenius/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ranks': [8, 9, 9, 10],\n",
       " 'runtime_limits': [1, 2, 4, 8],\n",
       " 'validation_loss': [0.5,\n",
       "  0.31434830230010957,\n",
       "  0.32694414019715223,\n",
       "  0.2993793355239138,\n",
       "  0.2993793355239138],\n",
       " 'filled_new_row': [array([[ 0.4005549 ,  0.40449643, -0.06828102, -0.21671784,  0.5       ,\n",
       "           0.40628271,  0.40292611, -0.09364233, -0.25991693, -0.2691485 ,\n",
       "           0.36892752,  0.10540695,  0.11637227,  0.11979539,  0.38773422,\n",
       "           0.19757196,  0.27535789,  0.3984177 ,  0.5       ,  0.5       ,\n",
       "           0.11151902,  0.10304746,  0.37300554,  0.1028667 ,  0.19317698,\n",
       "           0.21298227,  0.20069583,  0.21243705,  0.20367409,  0.22018121,\n",
       "           0.21023254,  0.231018  ,  0.22534233,  0.2493321 ,  0.23705593,\n",
       "           0.2665499 ,  0.29763796,  0.32814455,  0.35886272,  0.38703568,\n",
       "           0.39290791,  0.4002345 ,  0.36585182,  0.3627971 ,  0.17356417,\n",
       "           0.20194896,  0.20116007,  0.21927758,  0.19367047,  0.21376296,\n",
       "           0.19317698,  0.21298227,  0.19317698,  0.21298227, -0.29841796,\n",
       "          -0.07099534, -0.34439218, -0.13435899,  0.05137049,  0.20297192,\n",
       "           0.08187254,  0.18098037,  0.07426858,  0.22763069,  0.08627079,\n",
       "           0.18319244,  0.08029209,  0.21264894,  0.07024107,  0.15691279,\n",
       "           0.08157147,  0.18405059,  0.07070115,  0.13875682,  0.07974122,\n",
       "           0.18142643,  0.07731866,  0.14714549,  0.09408301,  0.22270372,\n",
       "           0.07933177,  0.20034945,  0.31452283,  0.39809004,  0.42023704,\n",
       "           0.4548428 ,  0.48207496,  0.46389703,  0.50216301,  0.47155683,\n",
       "           0.51671782,  0.47735014,  0.52326412,  0.48213949,  0.52746065,\n",
       "           0.48242301,  0.52994086,  0.47702261,  0.52656579,  0.31914938,\n",
       "           0.32064344,  0.30065641,  0.31698942,  0.31433193,  0.31974997,\n",
       "           0.30215531,  0.31524595,  0.31067313,  0.31756195,  0.3040342 ,\n",
       "           0.31524872,  0.30893731,  0.31654547,  0.30453195,  0.31509093,\n",
       "           0.30604527,  0.31426705,  0.30450196,  0.31431092,  0.30519383,\n",
       "           0.31432234,  0.30629355,  0.31496272,  0.30330483,  0.3145848 ,\n",
       "           0.30794581,  0.31483089,  0.30395914,  0.31237264,  0.30757016,\n",
       "           0.31505996,  0.36999415,  0.37012413,  0.35905356,  0.35838806,\n",
       "           0.35587055,  0.355679  ,  0.27261422,  0.27308609,  0.27722159,\n",
       "           0.27755151,  0.25631388,  0.25683876,  0.35816597,  0.15629042,\n",
       "           0.17288158,  0.15698986,  0.17378741,  0.16372075,  0.17727672,\n",
       "           0.17301982,  0.19070249,  0.19412809,  0.21940961,  0.22615366,\n",
       "           0.25359988,  0.30236709,  0.32321261,  0.42560788,  0.44532281,\n",
       "           0.46811683,  0.44691318,  0.3660217 ,  0.36277199,  0.14069684,\n",
       "           0.18125638,  0.15500375,  0.16790069,  0.15692774,  0.17290737,\n",
       "           0.15629042,  0.17288158,  0.15629042,  0.17288158,  0.62018822,\n",
       "           0.62018822,  0.58803256,  0.27496877,  0.61785019,  0.61785019,\n",
       "           0.63869265,  0.27233432,  0.54879457,  0.54879457,  0.65692684,\n",
       "           0.26636664,  0.49940419,  0.49940419,  0.64637689,  0.26427718,\n",
       "           0.4585809 ,  0.4585809 ,  0.63255823,  0.26279004,  0.38562996,\n",
       "           0.38562996,  0.60846461,  0.26328388,  0.34282767,  0.34282767,\n",
       "           0.59555069,  0.26102149,  0.3174351 ,  0.3174351 ,  0.58164501,\n",
       "           0.25886055,  0.30352949,  0.30352949,  0.56761787,  0.25496198,\n",
       "           0.32530651,  0.32006308,  0.3187157 ,  0.31812376,  0.3181248 ,\n",
       "           0.31944029,  0.31900744,  0.31720504,  0.32137539]]),\n",
       "  array([[ 0.67980528,  0.7085481 ,  0.59837968,  0.55547793,  0.50065048,\n",
       "           0.69038257,  0.70182265,  0.53343386,  0.49880107,  0.43764554,\n",
       "           0.37471852,  0.37630258,  0.37866336,  0.37271017,  0.37294219,\n",
       "           0.39718112,  0.36452917,  0.39391085,  0.50702869,  0.49422092,\n",
       "           0.36654079,  0.3738971 ,  0.37471852,  0.37471852,  0.2821255 ,\n",
       "           0.30933201,  0.28040967,  0.30201763,  0.27407246,  0.29949961,\n",
       "           0.25638753,  0.29286994,  0.24889298,  0.28898691,  0.21675545,\n",
       "           0.26727109,  0.49451952,  0.24435261,  0.22367701,  0.26831388,\n",
       "           0.30112558,  0.31118284,  0.29039957,  0.28799155,  0.44090365,\n",
       "           0.22359327,  0.25724517,  0.28522748,  0.27692107,  0.30844403,\n",
       "           0.2821255 ,  0.30933201,  0.2821255 ,  0.30933201,  0.5       ,\n",
       "          -0.10406147, -0.33133974, -0.20083268,  0.22398599,  0.26685901,\n",
       "           0.29665978,  0.32655425,  0.26255836,  0.3364584 ,  0.30967539,\n",
       "           0.34310905,  0.28443852,  0.35050053,  0.28512645,  0.33261081,\n",
       "           0.2967747 ,  0.35475506,  0.2922552 ,  0.33577224,  0.32147392,\n",
       "           0.39545906,  0.30526879,  0.36543873,  0.35246347,  0.46402613,\n",
       "           0.32121213,  0.44937683,  0.31743079,  0.41588796,  0.41922709,\n",
       "           0.42734178,  0.44046399,  0.42507558,  0.44476469,  0.43871787,\n",
       "           0.45827339,  0.44105414,  0.45312057,  0.43602099,  0.45727605,\n",
       "           0.44473636,  0.46219714,  0.42316516,  0.4423654 ,  0.24033207,\n",
       "           0.34612639,  0.21550687,  0.23385394,  0.24724659,  0.34152151,\n",
       "           0.2344707 ,  0.2378288 ,  0.24751388,  0.24571656,  0.24610208,\n",
       "           0.24327837,  0.24928358,  0.24904751,  0.25231896,  0.24561458,\n",
       "           0.25011115,  0.25345895,  0.25384881,  0.25040197,  0.25243564,\n",
       "           0.25733752,  0.25673365,  0.25233231,  0.2525206 ,  0.26491415,\n",
       "           0.26097866,  0.25330122,  0.25331057,  0.26553974,  0.25998273,\n",
       "           0.25587981,  0.0779371 ,  0.07788938,  0.14839403,  0.14806832,\n",
       "           0.17662869,  0.17619227,  0.22232323,  0.22155735,  0.20224029,\n",
       "           0.20250693,  0.23129719,  0.23458069,  0.35729043,  0.30518851,\n",
       "           0.32271397,  0.30074293,  0.32175926,  0.30269385,  0.31663435,\n",
       "           0.29414849,  0.32412518,  0.30412111,  0.34233938,  0.27890467,\n",
       "           0.31644245,  0.27707018,  0.31127152,  0.35318761,  0.3807008 ,\n",
       "           0.37782835,  0.36352238,  0.32046537,  0.31428246,  0.23990611,\n",
       "           0.29415908,  0.27686873,  0.29658551,  0.29961007,  0.32143267,\n",
       "           0.30518851,  0.32271397,  0.30518851,  0.32271397,  0.53499426,\n",
       "           0.53499426,  0.69832681,  0.21086253,  0.55777768,  0.55777768,\n",
       "           0.81845133,  0.21377282,  0.51099393,  0.51099393,  0.87187333,\n",
       "           0.21303892,  0.47015482,  0.47015482,  0.87124228,  0.21537723,\n",
       "           0.42096891,  0.42096891,  0.85375592,  0.21677711,  0.33611032,\n",
       "           0.33611032,  0.80165564,  0.22310548,  0.30488512,  0.30488512,\n",
       "           0.74674227,  0.22675934,  0.29321576,  0.29321576,  0.71478973,\n",
       "           0.2253717 ,  0.29106249,  0.29106249,  0.68887862,  0.22643071,\n",
       "           0.26032397,  0.26781614,  0.27321204,  0.2768952 ,  0.27794618,\n",
       "           0.2813849 ,  0.28682163,  0.28939612,  0.28966162]]),\n",
       "  array([[0.35626025, 0.35554345, 0.43494738, 0.47570849, 0.48958649,\n",
       "          0.35661751, 0.358218  , 0.43495084, 0.46997128, 0.48156264,\n",
       "          0.37105957, 0.37191048, 0.37123174, 0.37215258, 0.3689296 ,\n",
       "          0.37160401, 0.38896871, 0.43050019, 0.47762895, 0.4965923 ,\n",
       "          0.37352961, 0.37120745, 0.37105957, 0.37105957, 0.39147502,\n",
       "          0.39659648, 0.39290191, 0.39553895, 0.39568225, 0.39734071,\n",
       "          0.40036447, 0.4023829 , 0.40755327, 0.4115332 , 0.42118498,\n",
       "          0.42584621, 0.44013159, 0.44502317, 0.46974827, 0.47297042,\n",
       "          0.50106327, 0.50244762, 0.51399746, 0.51443127, 0.41350863,\n",
       "          0.41694785, 0.40101859, 0.40516339, 0.39176851, 0.39638634,\n",
       "          0.39147502, 0.39659648, 0.39147502, 0.39659648, 0.51963455,\n",
       "          0.54570718, 0.52446149, 0.55108754, 0.38931745, 0.46016696,\n",
       "          0.38173296, 0.43693694, 0.36637895, 0.41272217, 0.36782028,\n",
       "          0.4022727 , 0.35754417, 0.39049302, 0.36054115, 0.38671653,\n",
       "          0.35527206, 0.37909127, 0.35868932, 0.38190372, 0.35423737,\n",
       "          0.37288173, 0.35601757, 0.37784267, 0.3567759 , 0.38268406,\n",
       "          0.35989208, 0.38845643, 0.36128091, 0.4088756 , 0.40993242,\n",
       "          0.41545814, 0.42053482, 0.42353123, 0.42623462, 0.42808932,\n",
       "          0.42985151, 0.42804893, 0.43239466, 0.43034625, 0.43527498,\n",
       "          0.43407924, 0.43793781, 0.43632963, 0.44346799, 0.3474783 ,\n",
       "          0.33847334, 0.34239333, 0.3418124 , 0.329317  , 0.33709987,\n",
       "          0.33838953, 0.33954173, 0.34140568, 0.33567768, 0.35241832,\n",
       "          0.33856695, 0.32060381, 0.32204275, 0.35693293, 0.34347161,\n",
       "          0.32587524, 0.33035821, 0.3530418 , 0.36236121, 0.31717189,\n",
       "          0.32353915, 0.34809319, 0.34846356, 0.32334582, 0.31922422,\n",
       "          0.35414639, 0.35091038, 0.32289403, 0.32191548, 0.36171877,\n",
       "          0.39009349, 0.42295554, 0.42300226, 0.37889839, 0.37882345,\n",
       "          0.3809591 , 0.38096199, 0.36811081, 0.36834122, 0.36568259,\n",
       "          0.36562254, 0.37406535, 0.37456456, 0.34744293, 0.38889312,\n",
       "          0.38783876, 0.38937788, 0.387238  , 0.38813083, 0.39109854,\n",
       "          0.39088167, 0.39575189, 0.39903908, 0.40277773, 0.41097047,\n",
       "          0.41406849, 0.43892207, 0.44376444, 0.48030319, 0.4833948 ,\n",
       "          0.50417809, 0.50781697, 0.5182758 , 0.51904612, 0.40518647,\n",
       "          0.40984453, 0.39138807, 0.39085224, 0.38883895, 0.38810031,\n",
       "          0.38889312, 0.38783876, 0.38889312, 0.38783876, 0.50818812,\n",
       "          0.50818812, 0.51518318, 0.37057349, 0.47484205, 0.47484205,\n",
       "          0.50507281, 0.37141363, 0.44692407, 0.44692407, 0.48876122,\n",
       "          0.37089239, 0.4335336 , 0.4335336 , 0.4761143 , 0.37073129,\n",
       "          0.42256272, 0.42256272, 0.46740333, 0.37074028, 0.40221087,\n",
       "          0.40221087, 0.45092109, 0.36925351, 0.39407631, 0.39407631,\n",
       "          0.43814842, 0.3689834 , 0.39022295, 0.39022295, 0.43153897,\n",
       "          0.37084256, 0.38624266, 0.38624266, 0.42822798, 0.3717496 ,\n",
       "          0.34079348, 0.33941947, 0.33825938, 0.33815393, 0.39111502,\n",
       "          0.33838507, 0.39595208, 0.35341249, 0.341382  ]]),\n",
       "  array([[0.32027712, 0.33413504, 0.41498571, 0.47983244, 0.49149283,\n",
       "          0.32850606, 0.35605697, 0.41838818, 0.47735328, 0.48719346,\n",
       "          0.3684878 , 0.36894513, 0.36771502, 0.3690656 , 0.36640904,\n",
       "          0.36830874, 0.38675065, 0.43945514, 0.47937274, 0.48918658,\n",
       "          0.36973784, 0.36859109, 0.3684878 , 0.3684878 , 0.38727631,\n",
       "          0.3919375 , 0.38906544, 0.39043716, 0.3924814 , 0.3928838 ,\n",
       "          0.39824023, 0.39869767, 0.406969  , 0.40943415, 0.42424661,\n",
       "          0.42744401, 0.45021482, 0.45425161, 0.48432236, 0.4871618 ,\n",
       "          0.50670532, 0.50746473, 0.50878212, 0.5088181 , 0.40910417,\n",
       "          0.41121927, 0.39699709, 0.40003616, 0.38768861, 0.39186267,\n",
       "          0.38727631, 0.3919375 , 0.38727631, 0.3919375 , 0.5206706 ,\n",
       "          0.53959826, 0.52463198, 0.54498388, 0.38839642, 0.45518163,\n",
       "          0.3793266 , 0.4352682 , 0.36344237, 0.40594774, 0.36594512,\n",
       "          0.39872404, 0.35396773, 0.38365443, 0.35938838, 0.38344134,\n",
       "          0.35083376, 0.37195965, 0.35755837, 0.37905004, 0.3490067 ,\n",
       "          0.36447254, 0.35436264, 0.37475413, 0.35063661, 0.37229954,\n",
       "          0.35803085, 0.38265458, 0.35414189, 0.39619501, 0.39767856,\n",
       "          0.40029472, 0.40599706, 0.40978402, 0.41276644, 0.41501668,\n",
       "          0.41717273, 0.41604096, 0.42066216, 0.41854782, 0.42409038,\n",
       "          0.42229348, 0.42676054, 0.42417459, 0.43196347, 0.35423804,\n",
       "          0.34717691, 0.35032473, 0.3506758 , 0.3445824 , 0.34575435,\n",
       "          0.3464161 , 0.34806785, 0.34239492, 0.34406991, 0.34495705,\n",
       "          0.34689064, 0.34165457, 0.34392036, 0.34494806, 0.34627221,\n",
       "          0.34006687, 0.34321253, 0.34483377, 0.34607857, 0.33900068,\n",
       "          0.34266561, 0.34566262, 0.34602799, 0.33759225, 0.34197452,\n",
       "          0.34580885, 0.3461405 , 0.33777923, 0.34240791, 0.34529601,\n",
       "          0.34586754, 0.4169621 , 0.41699559, 0.38384542, 0.38377265,\n",
       "          0.38596295, 0.38597687, 0.36815895, 0.36839271, 0.36586182,\n",
       "          0.36583132, 0.37232889, 0.37274754, 0.35301822, 0.38523107,\n",
       "          0.38307941, 0.38594822, 0.38267595, 0.38517708, 0.3870539 ,\n",
       "          0.38839855, 0.39237463, 0.39796869, 0.40017605, 0.41241631,\n",
       "          0.41465134, 0.44959243, 0.45364834, 0.49403626, 0.49671308,\n",
       "          0.50160406, 0.50396349, 0.50537619, 0.50583149, 0.39997529,\n",
       "          0.40301873, 0.38745761, 0.38575152, 0.38529936, 0.38329045,\n",
       "          0.38523107, 0.38307941, 0.38523107, 0.38307941, 0.51477995,\n",
       "          0.51477995, 0.51500961, 0.36571738, 0.47706856, 0.47706856,\n",
       "          0.50787623, 0.36583384, 0.44073785, 0.44073785, 0.49281639,\n",
       "          0.36464152, 0.42561108, 0.42561108, 0.4793132 , 0.36420682,\n",
       "          0.41409525, 0.41409525, 0.46996509, 0.36427249, 0.39415975,\n",
       "          0.39415975, 0.45178072, 0.36266962, 0.38629748, 0.38629748,\n",
       "          0.43666605, 0.36212909, 0.38255103, 0.38255103, 0.42864724,\n",
       "          0.36390292, 0.37889084, 0.37889084, 0.42401232, 0.36460751,\n",
       "          0.31814251, 0.34736043, 0.3457465 , 0.34545769, 0.34523403,\n",
       "          0.34531999, 0.3448004 , 0.34422158, 0.34757979]])],\n",
       " 'predicted_new_row': [array([[ 0.4005549 ,  0.40449643, -0.06828102, -0.21671784, -0.23179748,\n",
       "           0.40628271,  0.40292611, -0.09364233, -0.25991693, -0.2691485 ,\n",
       "           0.1028667 ,  0.10540695,  0.11637227,  0.11979539,  0.14372007,\n",
       "           0.19757196,  0.27535789,  0.3984177 ,  0.5       ,  0.5       ,\n",
       "           0.11151902,  0.10304746,  0.1028667 ,  0.1028667 ,  0.19317698,\n",
       "           0.21298227,  0.20069583,  0.21243705,  0.20367409,  0.22018121,\n",
       "           0.21023254,  0.231018  ,  0.22534233,  0.2493321 ,  0.23705593,\n",
       "           0.2665499 ,  0.29763796,  0.32814455,  0.35886272,  0.38703568,\n",
       "           0.39290791,  0.4002345 ,  0.36585182,  0.3627971 ,  0.17356417,\n",
       "           0.20194896,  0.20116007,  0.21927758,  0.19367047,  0.21376296,\n",
       "           0.19317698,  0.21298227,  0.19317698,  0.21298227, -0.29841796,\n",
       "          -0.07099534, -0.34439218, -0.13435899,  0.05137049,  0.20297192,\n",
       "           0.08187254,  0.18098037,  0.07426858,  0.22763069,  0.08627079,\n",
       "           0.18319244,  0.08029209,  0.21264894,  0.07024107,  0.15691279,\n",
       "           0.08157147,  0.18405059,  0.07070115,  0.13875682,  0.07974122,\n",
       "           0.18142643,  0.07731866,  0.14714549,  0.09408301,  0.22270372,\n",
       "           0.07933177,  0.20034945,  0.31452283,  0.39809004,  0.42023704,\n",
       "           0.4548428 ,  0.48207496,  0.46389703,  0.50216301,  0.47155683,\n",
       "           0.51671782,  0.47735014,  0.52326412,  0.48213949,  0.52746065,\n",
       "           0.48242301,  0.52994086,  0.47702261,  0.52656579,  0.31914938,\n",
       "           0.32064344,  0.30065641,  0.31698942,  0.31433193,  0.31974997,\n",
       "           0.30215531,  0.31524595,  0.31067313,  0.31756195,  0.3040342 ,\n",
       "           0.31524872,  0.30893731,  0.31654547,  0.30453195,  0.31509093,\n",
       "           0.30604527,  0.31426705,  0.30450196,  0.31431092,  0.30519383,\n",
       "           0.31432234,  0.30629355,  0.31496272,  0.30330483,  0.3145848 ,\n",
       "           0.30794581,  0.31483089,  0.30395914,  0.31237264,  0.30757016,\n",
       "           0.31505996,  0.36999415,  0.37012413,  0.35905356,  0.35838806,\n",
       "           0.35587055,  0.355679  ,  0.27261422,  0.27308609,  0.27722159,\n",
       "           0.27755151,  0.25631388,  0.25683876,  0.35816597,  0.15629042,\n",
       "           0.17288158,  0.15698986,  0.17378741,  0.16372075,  0.17727672,\n",
       "           0.17301982,  0.19070249,  0.19412809,  0.21940961,  0.22615366,\n",
       "           0.25359988,  0.30236709,  0.32321261,  0.42560788,  0.44532281,\n",
       "           0.46811683,  0.44691318,  0.3660217 ,  0.36277199,  0.14069684,\n",
       "           0.18125638,  0.15500375,  0.16790069,  0.15692774,  0.17290737,\n",
       "           0.15629042,  0.17288158,  0.15629042,  0.17288158,  0.62018822,\n",
       "           0.62018822,  0.58803256,  0.27496877,  0.61785019,  0.61785019,\n",
       "           0.63869265,  0.27233432,  0.54879457,  0.54879457,  0.65692684,\n",
       "           0.26636664,  0.49940419,  0.49940419,  0.64637689,  0.26427718,\n",
       "           0.4585809 ,  0.4585809 ,  0.63255823,  0.26279004,  0.38562996,\n",
       "           0.38562996,  0.60846461,  0.26328388,  0.34282767,  0.34282767,\n",
       "           0.59555069,  0.26102149,  0.3174351 ,  0.3174351 ,  0.58164501,\n",
       "           0.25886055,  0.30352949,  0.30352949,  0.56761787,  0.25496198,\n",
       "           0.32530651,  0.32006308,  0.3187157 ,  0.31812376,  0.3181248 ,\n",
       "           0.31944029,  0.31900744,  0.31720504,  0.32137539]]),\n",
       "  array([[ 0.67980528,  0.7085481 ,  0.59837968,  0.55547793,  0.50065048,\n",
       "           0.69038257,  0.70182265,  0.53343386,  0.49880107,  0.43764554,\n",
       "           0.37471852,  0.37630258,  0.37866336,  0.37271017,  0.37294219,\n",
       "           0.39718112,  0.36452917,  0.39391085,  0.50702869,  0.49422092,\n",
       "           0.36654079,  0.3738971 ,  0.37471852,  0.37471852,  0.2821255 ,\n",
       "           0.30933201,  0.28040967,  0.30201763,  0.27407246,  0.29949961,\n",
       "           0.25638753,  0.29286994,  0.24889298,  0.28898691,  0.21675545,\n",
       "           0.26727109,  0.19512254,  0.24435261,  0.22367701,  0.26831388,\n",
       "           0.30112558,  0.31118284,  0.29039957,  0.28799155,  0.18443344,\n",
       "           0.22359327,  0.25724517,  0.28522748,  0.27692107,  0.30844403,\n",
       "           0.2821255 ,  0.30933201,  0.2821255 ,  0.30933201, -0.29361753,\n",
       "          -0.10406147, -0.33133974, -0.20083268,  0.22398599,  0.26685901,\n",
       "           0.29665978,  0.32655425,  0.26255836,  0.3364584 ,  0.30967539,\n",
       "           0.34310905,  0.28443852,  0.35050053,  0.28512645,  0.33261081,\n",
       "           0.2967747 ,  0.35475506,  0.2922552 ,  0.33577224,  0.32147392,\n",
       "           0.39545906,  0.30526879,  0.36543873,  0.35246347,  0.46402613,\n",
       "           0.32121213,  0.44937683,  0.31743079,  0.41588796,  0.41922709,\n",
       "           0.42734178,  0.44046399,  0.42507558,  0.44476469,  0.43871787,\n",
       "           0.45827339,  0.44105414,  0.45312057,  0.43602099,  0.45727605,\n",
       "           0.44473636,  0.46219714,  0.42316516,  0.4423654 ,  0.24033207,\n",
       "           0.23929759,  0.21550687,  0.23385394,  0.24724659,  0.24327217,\n",
       "           0.2344707 ,  0.2378288 ,  0.24751388,  0.24571656,  0.24610208,\n",
       "           0.24327837,  0.24928358,  0.24904751,  0.25231896,  0.24561458,\n",
       "           0.25011115,  0.25345895,  0.25384881,  0.25040197,  0.25243564,\n",
       "           0.25733752,  0.25673365,  0.25233231,  0.2525206 ,  0.26491415,\n",
       "           0.26097866,  0.25330122,  0.25331057,  0.26553974,  0.25998273,\n",
       "           0.25587981,  0.0779371 ,  0.07788938,  0.14839403,  0.14806832,\n",
       "           0.17662869,  0.17619227,  0.22232323,  0.22155735,  0.20224029,\n",
       "           0.20250693,  0.23129719,  0.23458069,  0.35729043,  0.30518851,\n",
       "           0.32271397,  0.30074293,  0.32175926,  0.30269385,  0.31663435,\n",
       "           0.29414849,  0.32412518,  0.30412111,  0.34233938,  0.27890467,\n",
       "           0.31644245,  0.27707018,  0.31127152,  0.35318761,  0.3807008 ,\n",
       "           0.37782835,  0.36352238,  0.32046537,  0.31428246,  0.23990611,\n",
       "           0.29415908,  0.27686873,  0.29658551,  0.29961007,  0.32143267,\n",
       "           0.30518851,  0.32271397,  0.30518851,  0.32271397,  0.53499426,\n",
       "           0.53499426,  0.69832681,  0.21086253,  0.55777768,  0.55777768,\n",
       "           0.81845133,  0.21377282,  0.51099393,  0.51099393,  0.87187333,\n",
       "           0.21303892,  0.47015482,  0.47015482,  0.87124228,  0.21537723,\n",
       "           0.42096891,  0.42096891,  0.85375592,  0.21677711,  0.33611032,\n",
       "           0.33611032,  0.80165564,  0.22310548,  0.30488512,  0.30488512,\n",
       "           0.74674227,  0.22675934,  0.29321576,  0.29321576,  0.71478973,\n",
       "           0.2253717 ,  0.29106249,  0.29106249,  0.68887862,  0.22643071,\n",
       "           0.26032397,  0.26781614,  0.27321204,  0.2768952 ,  0.27794618,\n",
       "           0.2813849 ,  0.28682163,  0.28939612,  0.28966162]]),\n",
       "  array([[0.35626025, 0.35554345, 0.43494738, 0.47570849, 0.48958649,\n",
       "          0.35661751, 0.358218  , 0.43495084, 0.46997128, 0.48156264,\n",
       "          0.37105957, 0.37191048, 0.37123174, 0.37215258, 0.3689296 ,\n",
       "          0.37160401, 0.38896871, 0.43050019, 0.47762895, 0.4965923 ,\n",
       "          0.37352961, 0.37120745, 0.37105957, 0.37105957, 0.39147502,\n",
       "          0.39659648, 0.39290191, 0.39553895, 0.39568225, 0.39734071,\n",
       "          0.40036447, 0.4023829 , 0.40755327, 0.4115332 , 0.42118498,\n",
       "          0.42584621, 0.44013159, 0.44502317, 0.46974827, 0.47297042,\n",
       "          0.50106327, 0.50244762, 0.51399746, 0.51443127, 0.41350863,\n",
       "          0.41694785, 0.40101859, 0.40516339, 0.39176851, 0.39638634,\n",
       "          0.39147502, 0.39659648, 0.39147502, 0.39659648, 0.51963455,\n",
       "          0.54570718, 0.52446149, 0.55108754, 0.38931745, 0.46016696,\n",
       "          0.38173296, 0.43693694, 0.36637895, 0.41272217, 0.36782028,\n",
       "          0.4022727 , 0.35754417, 0.39049302, 0.36054115, 0.38671653,\n",
       "          0.35527206, 0.37909127, 0.35868932, 0.38190372, 0.35423737,\n",
       "          0.37288173, 0.35601757, 0.37784267, 0.3567759 , 0.38268406,\n",
       "          0.35989208, 0.38845643, 0.36128091, 0.4088756 , 0.40993242,\n",
       "          0.41545814, 0.42053482, 0.42353123, 0.42623462, 0.42808932,\n",
       "          0.42985151, 0.42804893, 0.43239466, 0.43034625, 0.43527498,\n",
       "          0.43407924, 0.43793781, 0.43632963, 0.44346799, 0.3474783 ,\n",
       "          0.33847334, 0.34239333, 0.3418124 , 0.33821074, 0.33709987,\n",
       "          0.33838953, 0.33954173, 0.33628694, 0.33567768, 0.33706556,\n",
       "          0.33856695, 0.3355531 , 0.33554266, 0.33704598, 0.33805632,\n",
       "          0.33418514, 0.33501852, 0.33683777, 0.33801904, 0.33338645,\n",
       "          0.33458539, 0.33767383, 0.33794999, 0.33194   , 0.33403047,\n",
       "          0.33780348, 0.33814641, 0.33217797, 0.33449413, 0.3372718 ,\n",
       "          0.33792531, 0.42295554, 0.42300226, 0.37889839, 0.37882345,\n",
       "          0.3809591 , 0.38096199, 0.36811081, 0.36834122, 0.36568259,\n",
       "          0.36562254, 0.37406535, 0.37456456, 0.34744293, 0.38889312,\n",
       "          0.38783876, 0.38937788, 0.387238  , 0.38813083, 0.39109854,\n",
       "          0.39088167, 0.39575189, 0.39903908, 0.40277773, 0.41097047,\n",
       "          0.41406849, 0.43892207, 0.44376444, 0.48030319, 0.4833948 ,\n",
       "          0.50417809, 0.50781697, 0.5182758 , 0.51904612, 0.40518647,\n",
       "          0.40984453, 0.39138807, 0.39085224, 0.38883895, 0.38810031,\n",
       "          0.38889312, 0.38783876, 0.38889312, 0.38783876, 0.50818812,\n",
       "          0.50818812, 0.51518318, 0.37057349, 0.47484205, 0.47484205,\n",
       "          0.50507281, 0.37141363, 0.44692407, 0.44692407, 0.48876122,\n",
       "          0.37089239, 0.4335336 , 0.4335336 , 0.4761143 , 0.37073129,\n",
       "          0.42256272, 0.42256272, 0.46740333, 0.37074028, 0.40221087,\n",
       "          0.40221087, 0.45092109, 0.36925351, 0.39407631, 0.39407631,\n",
       "          0.43814842, 0.3689834 , 0.39022295, 0.39022295, 0.43153897,\n",
       "          0.37084256, 0.38624266, 0.38624266, 0.42822798, 0.3717496 ,\n",
       "          0.34079348, 0.33941947, 0.33825938, 0.33815393, 0.33799396,\n",
       "          0.33838507, 0.33812384, 0.33770979, 0.341382  ]]),\n",
       "  array([[0.33010649, 0.32753498, 0.41498571, 0.47983244, 0.49149283,\n",
       "          0.32961961, 0.33020501, 0.41838818, 0.47735328, 0.48719346,\n",
       "          0.3684878 , 0.36894513, 0.36771502, 0.3690656 , 0.36640904,\n",
       "          0.36830874, 0.38675065, 0.43945514, 0.47937274, 0.48918658,\n",
       "          0.36973784, 0.36859109, 0.3684878 , 0.3684878 , 0.38727631,\n",
       "          0.3919375 , 0.38906544, 0.39043716, 0.3924814 , 0.3928838 ,\n",
       "          0.39824023, 0.39869767, 0.406969  , 0.40943415, 0.42424661,\n",
       "          0.42744401, 0.45021482, 0.45425161, 0.48432236, 0.4871618 ,\n",
       "          0.50670532, 0.50746473, 0.50878212, 0.5088181 , 0.40910417,\n",
       "          0.41121927, 0.39699709, 0.40003616, 0.38768861, 0.39186267,\n",
       "          0.38727631, 0.3919375 , 0.38727631, 0.3919375 , 0.5206706 ,\n",
       "          0.53959826, 0.52463198, 0.54498388, 0.38839642, 0.45518163,\n",
       "          0.3793266 , 0.4352682 , 0.36344237, 0.40594774, 0.36594512,\n",
       "          0.39872404, 0.35396773, 0.38365443, 0.35938838, 0.38344134,\n",
       "          0.35083376, 0.37195965, 0.35755837, 0.37905004, 0.3490067 ,\n",
       "          0.36447254, 0.35436264, 0.37475413, 0.35063661, 0.37229954,\n",
       "          0.35803085, 0.38265458, 0.35414189, 0.39619501, 0.39767856,\n",
       "          0.40029472, 0.40599706, 0.40978402, 0.41276644, 0.41501668,\n",
       "          0.41717273, 0.41604096, 0.42066216, 0.41854782, 0.42409038,\n",
       "          0.42229348, 0.42676054, 0.42417459, 0.43196347, 0.35423804,\n",
       "          0.34717691, 0.35032473, 0.3506758 , 0.3445824 , 0.34575435,\n",
       "          0.3464161 , 0.34806785, 0.34239492, 0.34406991, 0.34495705,\n",
       "          0.34689064, 0.34165457, 0.34392036, 0.34494806, 0.34627221,\n",
       "          0.34006687, 0.34321253, 0.34483377, 0.34607857, 0.33900068,\n",
       "          0.34266561, 0.34566262, 0.34602799, 0.33759225, 0.34197452,\n",
       "          0.34580885, 0.3461405 , 0.33777923, 0.34240791, 0.34529601,\n",
       "          0.34586754, 0.4169621 , 0.41699559, 0.38384542, 0.38377265,\n",
       "          0.38596295, 0.38597687, 0.36815895, 0.36839271, 0.36586182,\n",
       "          0.36583132, 0.37232889, 0.37274754, 0.35301822, 0.38523107,\n",
       "          0.38307941, 0.38594822, 0.38267595, 0.38517708, 0.3870539 ,\n",
       "          0.38839855, 0.39237463, 0.39796869, 0.40017605, 0.41241631,\n",
       "          0.41465134, 0.44959243, 0.45364834, 0.49403626, 0.49671308,\n",
       "          0.50160406, 0.50396349, 0.50537619, 0.50583149, 0.39997529,\n",
       "          0.40301873, 0.38745761, 0.38575152, 0.38529936, 0.38329045,\n",
       "          0.38523107, 0.38307941, 0.38523107, 0.38307941, 0.51477995,\n",
       "          0.51477995, 0.51500961, 0.36571738, 0.47706856, 0.47706856,\n",
       "          0.50787623, 0.36583384, 0.44073785, 0.44073785, 0.49281639,\n",
       "          0.36464152, 0.42561108, 0.42561108, 0.4793132 , 0.36420682,\n",
       "          0.41409525, 0.41409525, 0.46996509, 0.36427249, 0.39415975,\n",
       "          0.39415975, 0.45178072, 0.36266962, 0.38629748, 0.38629748,\n",
       "          0.43666605, 0.36212909, 0.38255103, 0.38255103, 0.42864724,\n",
       "          0.36390292, 0.37889084, 0.37889084, 0.42401232, 0.36460751,\n",
       "          0.34908816, 0.34736043, 0.3457465 , 0.34545769, 0.34523403,\n",
       "          0.34531999, 0.3448004 , 0.34422158, 0.34757979]])],\n",
       " 'actual_runtimes': [1.5661942958831787,\n",
       "  4.607572555541992,\n",
       "  9.324779272079468,\n",
       "  15.643882751464844],\n",
       " 'sampled_indices': [{4, 10, 14, 17, 18, 19, 22, 82, 84, 143},\n",
       "  {4, 10, 14, 16, 17, 18, 19, 22, 36, 44, 54, 82, 84, 97, 100, 104, 143},\n",
       "  {4,\n",
       "   10,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   36,\n",
       "   44,\n",
       "   54,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   97,\n",
       "   100,\n",
       "   103,\n",
       "   104,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   143,\n",
       "   184,\n",
       "   214,\n",
       "   216,\n",
       "   217},\n",
       "  {0,\n",
       "   1,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   10,\n",
       "   11,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   36,\n",
       "   38,\n",
       "   44,\n",
       "   54,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   88,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   100,\n",
       "   103,\n",
       "   104,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   143,\n",
       "   163,\n",
       "   184,\n",
       "   210,\n",
       "   214,\n",
       "   216,\n",
       "   217}],\n",
       " 'models': [<oboe.ensemble.Ensemble at 0x7efe52bbc6a0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7efe52bbc6a0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7efe52bbc6a0>,\n",
       "  <oboe.ensemble.Ensemble at 0x7efe52bbc6a0>]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"tpot score:\\t\\t \", clf_german.score(X_test, y_test))\n",
    "\n",
    "x_test = np.array(X_test)\n",
    "yy_test = np.array(y_test)\n",
    "y_predicted = clf_oboe.predict(x_test)\n",
    "y_predicted = y_predicted.reshape((yy_test.size,))\n",
    "print(\"oboe score:\\t\\t \", accuracy_score(yy_test, y_predicted) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tpot score:\t\t  0.7604166666666666\n",
      "oboe score:\t\t  0.7239583333333334\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print(\"sklearn score:\\t\\t \", clf_autosklearn.score(X_test, y_test))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sklearn score:\t\t  0.875\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(\"oboe score:\\t\\t \", accuracy_score(yy_test, y_predicted) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "oboe score:\t\t  0.7239583333333334\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Save models \n",
    "import pickle\n",
    "pickle.dump(clf_autosklearn, open('autosklearn_german.sav', 'wb'))\n",
    "pickle.dump(clf_oboe, open('oboe_german.sav', 'wb'))\n",
    "clf_german.export('tpot_german.sav')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Load saved models \n",
    "import pickle \n",
    "clf_autosklearn = pickle.load(open('autosklearn_german.sav', 'rb'))\n",
    "clf_oboe = pickle.load(open('oboe_german.sav', 'rb'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.5674825237417542\n",
    "exported_pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n",
    "    LogisticRegression(C=10.0, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "tpot_pred = exported_pipeline.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ensemble learning hard vote \n",
    "sklearn_pred = clf_autosklearn.predict(X_test)\n",
    "tpot_pred = clf_german.predict(X_test)\n",
    "oboe_pred = clf_oboe.predict(X_test)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test,sklearn_pred ))\n",
    "print(accuracy_score(y_test,oboe_pred ))\n",
    "print(accuracy_score(y_test,tpot_pred ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.875\n",
      "0.71875\n",
      "0.7552083333333334\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Construct pandas dataframe with predicted classes from each tool\n",
    "prob = pd.DataFrame(list(zip(sklearn_pred, oboe_pred, tpot_pred)), columns=[ \"AutoSklearn_Class\", \"Oboe_Class\", \"TPOT_Class\"])\n",
    "\n",
    "# Add column ensemble (the majority vote or the best score == Autosklearn)\n",
    "def function(x) :\n",
    "    if [x['AutoSklearn_Class'],x['Oboe_Class'], x['TPOT_Class']].count(1.0)>=2 :\n",
    "        return 1\n",
    "    elif  [x['AutoSklearn_Class'],x['Oboe_Class'], x['TPOT_Class']].count(2.0) >= 2 :\n",
    "        return 2\n",
    "    else :\n",
    "        int(x['AutoSklearn_Class'])\n",
    "prob['ensemble'] = prob.apply(function, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "print(accuracy_score(y_test.astype(int), prob['ensemble'] ))\n",
    "# Its is not better :/ "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7916666666666666\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "prob"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     AutoSklearn_Class  Oboe_Class  TPOT_Class  ensemble\n",
       "0                  1.0         1.0         1.0         1\n",
       "1                  1.0         1.0         1.0         1\n",
       "2                  1.0         1.0         1.0         1\n",
       "3                  1.0         1.0         1.0         1\n",
       "4                  2.0         2.0         2.0         2\n",
       "..                 ...         ...         ...       ...\n",
       "187                1.0         1.0         1.0         1\n",
       "188                1.0         2.0         2.0         2\n",
       "189                1.0         1.0         1.0         1\n",
       "190                1.0         1.0         1.0         1\n",
       "191                1.0         1.0         1.0         1\n",
       "\n",
       "[192 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AutoSklearn_Class</th>\n",
       "      <th>Oboe_Class</th>\n",
       "      <th>TPOT_Class</th>\n",
       "      <th>ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# Soft vote with tpot and autosklearn \n",
    "tpot_sklearn_prob = clf_german.predict_proba(X_test)\n",
    "auto_sklearn_prob = clf_autosklearn.predict_proba(X_test)\n",
    "AutoSklearn_Class=[]\n",
    "AutoSklearn_Prob= []\n",
    "TPOT_Class=[]\n",
    "TPOT_Prob= []\n",
    "for  value in auto_sklearn_prob :\n",
    "    AutoSklearn_Class.append(int(np.argmax(value)+1))\n",
    "    AutoSklearn_Prob.append(float(value[np.argmax(value)]))\n",
    "\n",
    "for  value in tpot_sklearn_prob :\n",
    "    TPOT_Class.append(int(np.argmax(value))+1)\n",
    "    TPOT_Prob.append(float(value[np.argmax(value)])) \n",
    "\n",
    "prob = pd.DataFrame(list(zip(AutoSklearn_Class, AutoSklearn_Prob, TPOT_Class, TPOT_Prob)), columns=[ \"AutoSklearn_Class\", \"AutoSklearn_Prob\", \"TPOT_Class\", \"TPOT_Prob\"])\n",
    "prob['ensemble'] = prob.apply(lambda x : \"T\" if x['AutoSklearn_Prob']<= x['TPOT_Prob'] else \"A\", axis=1)\n",
    "prob.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   AutoSklearn_Class  AutoSklearn_Prob  TPOT_Class  TPOT_Prob ensemble\n",
       "0                  1          0.885758           1   0.990230        T\n",
       "1                  1          0.521027           1   0.952191        T\n",
       "2                  2          0.629851           2   0.743273        T\n",
       "3                  2          0.505620           2   0.562200        T\n",
       "4                  2          0.531852           1   0.615180        T"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AutoSklearn_Class</th>\n",
       "      <th>AutoSklearn_Prob</th>\n",
       "      <th>TPOT_Class</th>\n",
       "      <th>TPOT_Prob</th>\n",
       "      <th>ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.885758</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990230</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.521027</td>\n",
       "      <td>1</td>\n",
       "      <td>0.952191</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.629851</td>\n",
       "      <td>2</td>\n",
       "      <td>0.743273</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.505620</td>\n",
       "      <td>2</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.531852</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615180</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "prob[\"ensemble\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    131\n",
       "2     61\n",
       "Name: ensemble, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "y_test.astype(int)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "480    1\n",
       "670    1\n",
       "614    2\n",
       "269    1\n",
       "899    2\n",
       "      ..\n",
       "589    2\n",
       "34     1\n",
       "469    1\n",
       "681    1\n",
       "879    1\n",
       "Name: 24, Length: 192, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test.astype(int), prob['ensemble'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f0305a4bd922d78a1a9933c10651bf50f37f6183f11a48783a3fae819d3386ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}